#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è Knowledge Graph Embeddings (PyKEEN)

–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Ä–∞–±–æ—Ç—É –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏:
1. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
2. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–≤—è–∑–µ–π (link prediction)
3. –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π
4. –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
"""

from owlready2 import *
import os
from sparql_queries import clean_name_for_display

try:
    from pykeen.models import TransE
    from pykeen.triples import TriplesFactory
    import torch
    HAS_PYKEEN = True
except ImportError:
    HAS_PYKEEN = False
    print("‚ùå PyKEEN –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install pykeen")


def load_pykeen_model(model_dir: str = "kg_embeddings_pykeen"):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å PyKEEN"""
    if not HAS_PYKEEN:
        return None
    
    print("="*80)
    print("–ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò PyKEEN")
    print("="*80)
    
    try:
        # –°–ø–æ—Å–æ–± 1: –ß–µ—Ä–µ–∑ load_model (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
        try:
            from pykeen import load_model
            model = load_model(model_dir)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º triples_factory
            triples_factory_path = os.path.join(model_dir, "training_triples")
            if os.path.exists(triples_factory_path):
                triples_factory = TriplesFactory.load(triples_factory_path)
            else:
                triples_factory = None
            
            print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {model_dir}")
            print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å embeddings: {model.embedding_dim}")
            print(f"   –°—É—â–Ω–æ—Å—Ç–µ–π: {model.num_entities}")
            print(f"   –û—Ç–Ω–æ—à–µ–Ω–∏–π: {model.num_relations}")
            
            return model, triples_factory
        except ImportError:
            pass
        
        # –°–ø–æ—Å–æ–± 2: –ó–∞–≥—Ä—É–∂–∞–µ–º —á–µ—Ä–µ–∑ —Ñ–∞–π–ª—ã –Ω–∞–ø—Ä—è–º—É—é
        # –ß–∏—Ç–∞–µ–º metadata –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏
        metadata_path = os.path.join(model_dir, "metadata.json")
        import json
        try:
            with open(metadata_path, 'r') as f:
                metadata = json.load(f)
        except:
            metadata = {}
        
        model_name = metadata.get('model', {}).get('model', 'TransE') if metadata else 'TransE'
        model_kwargs = metadata.get('model', {}).get('model_kwargs', {}) if metadata else {'embedding_dim': 64}
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º triples_factory –∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        triples_factory_path = os.path.join(model_dir, "training_triples")
        if os.path.exists(triples_factory_path):
            # PyKEEN —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç triples_factory –≤ –±–∏–Ω–∞—Ä–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
            try:
                triples_factory = TriplesFactory.from_path_binary(triples_factory_path)
            except:
                # –ï—Å–ª–∏ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, —Å–æ–∑–¥–∞–µ–º –∏–∑ TSV —Ñ–∞–π–ª–æ–≤
                numeric_triples_path = os.path.join(triples_factory_path, "numeric_triples.tsv.gz")
                entity_to_id_path = os.path.join(triples_factory_path, "entity_to_id.tsv.gz")
                relation_to_id_path = os.path.join(triples_factory_path, "relation_to_id.tsv.gz")
                
                if all(os.path.exists(p) for p in [numeric_triples_path, entity_to_id_path, relation_to_id_path]):
                    triples_factory = TriplesFactory.from_path(
                        path=numeric_triples_path,
                        entity_to_id_path=entity_to_id_path,
                        relation_to_id_path=relation_to_id_path
                    )
                else:
                    triples_factory = None
        else:
            triples_factory = None
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å —Ç–µ–º–∏ –∂–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        if model_name == "TransE":
            from pykeen.models import TransE
            if triples_factory:
                model = TransE(
                    triples_factory=triples_factory,
                    **model_kwargs
                )
            else:
                # –ï—Å–ª–∏ triples_factory –Ω–µ—Ç, —Å–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å –∏–∑–≤–µ—Å—Ç–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
                num_entities = model_kwargs.get('num_entities', 13379)
                num_relations = model_kwargs.get('num_relations', 6)
                embedding_dim = model_kwargs.get('embedding_dim', 64)
                model = TransE(
                    num_entities=num_entities,
                    num_relations=num_relations,
                    embedding_dim=embedding_dim
                )
        else:
            print(f"‚ö†Ô∏è  –ú–æ–¥–µ–ª—å {model_name} - –∏—Å–ø–æ–ª—å–∑—É–µ–º TransE –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
            from pykeen.models import TransE
            if triples_factory:
                model = TransE(triples_factory=triples_factory, embedding_dim=64)
            else:
                model = TransE(num_entities=13379, num_relations=6, embedding_dim=64)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏
        model_path = os.path.join(model_dir, "trained_model.pkl")
        if os.path.exists(model_path):
            try:
                import torch
                # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å weights_only=False (PyKEEN –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–ª–æ–∂–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã)
                loaded = torch.load(model_path, map_location='cpu', weights_only=False)
                
                if isinstance(loaded, dict):
                    if 'state_dict' in loaded:
                        model.load_state_dict(loaded['state_dict'])
                    else:
                        model.load_state_dict(loaded)
                elif hasattr(loaded, 'load_state_dict'):
                    # –≠—Ç–æ –ø–æ–ª–Ω–∞—è –º–æ–¥–µ–ª—å
                    model = loaded
                else:
                    model.load_state_dict(loaded)
            except Exception as e:
                print(f"   ‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏: {e}")
        
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {model_dir}")
        
        # –ü–æ–ª—É—á–∞–µ–º –∞—Ç—Ä–∏–±—É—Ç—ã –º–æ–¥–µ–ª–∏ (–≤ PyKEEN –æ–Ω–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –≤ —Ä–∞–∑–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö)
        embedding_dim = getattr(model, 'embedding_dim', None) or getattr(model, '_embedding_dim', None) or model_kwargs.get('embedding_dim', 64)
        num_entities = getattr(model, 'num_entities', None) or (triples_factory.num_entities if triples_factory else 13379)
        num_relations = getattr(model, 'num_relations', None) or (triples_factory.num_relations if triples_factory else 6)
        
        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å embeddings: {embedding_dim}")
        print(f"   –°—É—â–Ω–æ—Å—Ç–µ–π: {num_entities}")
        print(f"   –û—Ç–Ω–æ—à–µ–Ω–∏–π: {num_relations}")
        
        return model, triples_factory
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        import traceback
        traceback.print_exc()
        return None, None


def load_entity_mappings(mapping_dir: str = "kg_embeddings_pykeen"):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–∞–ø–ø–∏–Ω–≥–∏ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏ –æ—Ç–Ω–æ—à–µ–Ω–∏–π"""
    import json
    
    entity_to_id = {}
    relation_to_id = {}
    id_to_entity = {}
    id_to_relation = {}
    
    try:
        # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ triples_factory
        triples_factory_path = os.path.join(mapping_dir, "training_triples")
        if os.path.exists(triples_factory_path):
            try:
                triples_factory = TriplesFactory.load(triples_factory_path)
                # –ü–æ–ª—É—á–∞–µ–º –º–∞–ø–ø–∏–Ω–≥–∏ –∏–∑ triples_factory
                for idx, entity in enumerate(triples_factory.entity_to_id.keys()):
                    entity_to_id[entity] = idx
                for idx, rel in enumerate(triples_factory.relation_to_id.keys()):
                    relation_to_id[rel] = idx
            except:
                pass
        
        # –ï—Å–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–æ—Å—å, –ø—Ä–æ–±—É–µ–º –∏–∑ JSON
        if not entity_to_id:
            entity_file = os.path.join(mapping_dir, "entity_to_id.json")
            relation_file = os.path.join(mapping_dir, "relation_to_id.json")
            
            if os.path.exists(entity_file):
                with open(entity_file, 'r', encoding='utf-8') as f:
                    entity_to_id = json.load(f)
            
            if os.path.exists(relation_file):
                with open(relation_file, 'r', encoding='utf-8') as f:
                    relation_to_id = json.load(f)
        
        # –°–æ–∑–¥–∞–µ–º –æ–±—Ä–∞—Ç–Ω—ã–µ –º–∞–ø–ø–∏–Ω–≥–∏
        id_to_entity = {v: k for k, v in entity_to_id.items()}
        id_to_relation = {v: k for k, v in relation_to_id.items()}
        
        if entity_to_id:
            print(f"‚úÖ –ú–∞–ø–ø–∏–Ω–≥–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã:")
            print(f"   –°—É—â–Ω–æ—Å—Ç–µ–π: {len(entity_to_id)}")
            print(f"   –û—Ç–Ω–æ—à–µ–Ω–∏–π: {len(relation_to_id)}")
        else:
            print(f"‚ö†Ô∏è  –ú–∞–ø–ø–∏–Ω–≥–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å –Ω–∞–ø—Ä—è–º—É—é")
        
        return entity_to_id, relation_to_id, id_to_entity, id_to_relation
    except Exception as e:
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–∞–ø–ø–∏–Ω–≥–æ–≤: {e}")
        return {}, {}, {}, {}


def predict_tail(model, head_name: str, relation_name: str,
                entity_to_id: dict, relation_to_id: dict, id_to_entity: dict,
                top_k: int = 10):
    """
    –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï TAIL –¥–ª—è (head, relation, ?)
    
    –ü—Ä–∏–º–µ—Ä: (BMW_3_Series_2016, MadeBy, ?) ‚Üí –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è
    """
    if head_name not in entity_to_id or relation_name not in relation_to_id:
        print(f"‚ùå –°—É—â–Ω–æ—Å—Ç—å –∏–ª–∏ –æ—Ç–Ω–æ—à–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        return []
    
    head_id = entity_to_id[head_name]
    relation_id = relation_to_id[relation_name]
    
    print(f"\nüîç –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: ({head_name}, {relation_name}, ?)")
    print(f"   Head ID: {head_id}, Relation ID: {relation_id}")
    
    num_entities = model.num_entities
    
    hrt_batch = torch.zeros((num_entities, 3), dtype=torch.long)
    hrt_batch[:, 0] = head_id
    hrt_batch[:, 1] = relation_id
    hrt_batch[:, 2] = torch.arange(num_entities)
    
    with torch.no_grad():
        scores = model.score_hrt(hrt_batch)
    
    scores = scores.squeeze(-1)
    
    top_scores, top_indices = torch.topk(scores, k=min(top_k, len(scores)))
    tail_ids = top_indices.tolist()
    scores_list = top_scores.tolist()
    
    results = []
    for tail_id, score in zip(tail_ids, scores_list):
        tail_name = id_to_entity.get(int(tail_id), f"Entity_{int(tail_id)}")
        results.append((tail_name, float(score)))
    
    return results


def predict_head(model, relation_name: str, tail_name: str,
                entity_to_id: dict, relation_to_id: dict, id_to_entity: dict,
                top_k: int = 10):
    """
    –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï HEAD –¥–ª—è (?, relation, tail)
    
    –ü—Ä–∏–º–µ—Ä: (?, MadeBy, BMW) ‚Üí –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –≤—Å–µ –∞–≤—Ç–æ–º–æ–±–∏–ª–∏ –æ—Ç BMW
    """
    if tail_name not in entity_to_id or relation_name not in relation_to_id:
        print(f"‚ùå –°—É—â–Ω–æ—Å—Ç—å –∏–ª–∏ –æ—Ç–Ω–æ—à–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        return []
    
    tail_id = entity_to_id[tail_name]
    relation_id = relation_to_id[relation_name]
    
    print(f"\nüîç –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: (?, {relation_name}, {tail_name})")
    print(f"   Tail ID: {tail_id}, Relation ID: {relation_id}")
    
    num_entities = model.num_entities
    
    hrt_batch = torch.zeros((num_entities, 3), dtype=torch.long)
    hrt_batch[:, 0] = torch.arange(num_entities)
    hrt_batch[:, 1] = relation_id
    hrt_batch[:, 2] = tail_id
    
    with torch.no_grad():
        scores = model.score_hrt(hrt_batch)
    
    scores = scores.squeeze(-1)
    
    top_scores, top_indices = torch.topk(scores, k=min(top_k, len(scores)))
    head_ids = top_indices.tolist()
    scores_list = top_scores.tolist()
    
    results = []
    for head_id, score in zip(head_ids, scores_list):
        head_name_result = id_to_entity.get(int(head_id), f"Entity_{int(head_id)}")
        results.append((head_name_result, float(score)))
    
    return results


def find_similar_entities(model, entity_name: str, entity_to_id: dict, id_to_entity: dict, top_k: int = 10):
    """–ù–∞—Ö–æ–¥–∏—Ç –ø–æ—Ö–æ–∂–∏–µ —Å—É—â–Ω–æ—Å—Ç–∏ –ø–æ embedding"""
    if entity_name not in entity_to_id:
        return []
    
    entity_id = entity_to_id[entity_name]
    
    with torch.no_grad():
        all_entity_ids = torch.arange(model.num_entities, dtype=torch.long)
        all_embeddings = model.entity_representations[0](all_entity_ids)
        
        entity_embedding = all_embeddings[entity_id]
        
        similarities = torch.nn.functional.cosine_similarity(
            entity_embedding.unsqueeze(0), all_embeddings, dim=1
        )
    
    top_scores, top_indices = torch.topk(similarities, k=min(top_k + 1, len(similarities)), dim=-1)
    
    results = []
    for score, idx in zip(top_scores, top_indices):
        idx_val = int(idx.item())
        if idx_val != entity_id:
            similar_name = id_to_entity.get(idx_val, f"Entity_{idx_val}")
            results.append((similar_name, float(score.item())))
            if len(results) >= top_k:
                break
    
    return results


def display_results(results, onto, query_type: str = "prediction"):
    """–í—ã–≤–æ–¥–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ–± –∞–≤—Ç–æ–º–æ–±–∏–ª—è—Ö"""
    print("\n" + "="*80)
    print(f"–†–ï–ó–£–õ–¨–¢–ê–¢–´: –Ω–∞–π–¥–µ–Ω–æ {len(results)} —Å—É—â–Ω–æ—Å—Ç–µ–π")
    print("="*80)
    
    for idx, (entity_name, score) in enumerate(results, 1):
        print(f"\n{idx}. {clean_name_for_display(entity_name)}")
        print(f"   Score: {score:.4f}")
        
        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –≤ –æ–Ω—Ç–æ–ª–æ–≥–∏–∏
        vehicle = None
        for v in onto.Vehicle.instances():
            if v.name == entity_name:
                vehicle = v
                break
        
        if vehicle:
            print(f"   üìã –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:")
            if hasattr(vehicle, 'MadeBy') and vehicle.MadeBy:
                print(f"      –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å: {clean_name_for_display(vehicle.MadeBy[0].name)}")
            if hasattr(vehicle, 'Year') and vehicle.Year:
                print(f"      –ì–æ–¥: {vehicle.Year}")
            if hasattr(vehicle, 'MSRP') and vehicle.MSRP:
                print(f"      –¶–µ–Ω–∞: ${vehicle.MSRP:,.0f}")


def demonstrate_link_prediction(model, entity_to_id, relation_to_id, id_to_entity, onto):
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–≤—è–∑–µ–π"""
    print("\n" + "="*80)
    print("–î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø 1: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–≤—è–∑–µ–π (Link Prediction)")
    print("="*80)
    
    # –ü—Ä–∏–º–µ—Ä 1: –ù–∞–π—Ç–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è –∞–≤—Ç–æ–º–æ–±–∏–ª—è
    print("\nüìù –ü—Ä–∏–º–µ—Ä 1: –ö—Ç–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç —ç—Ç–æ—Ç –∞–≤—Ç–æ–º–æ–±–∏–ª—å?")
    print("   –ó–∞–ø—Ä–æ—Å: (BMW_3_Series_2016_145, MadeBy, ?)")
    
    # –ò—â–µ–º Vehicle –≤ –∏–Ω–¥–µ–∫—Å–µ
    vehicle_name = None
    for name in entity_to_id.keys():
        if 'BMW' in name and '3_Series' in name and '2016' in name:
            vehicle_name = name
            break
    
    if vehicle_name:
        results = predict_tail(model, vehicle_name, 'MadeBy', 
                             entity_to_id, relation_to_id, id_to_entity, top_k=5)
        display_results(results, onto)
    else:
        print("   ‚ö†Ô∏è  BMW 3 Series 2016 –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–π Vehicle")
        vehicle_name = list(entity_to_id.keys())[0]
        if 'MadeBy' in relation_to_id:
            results = predict_tail(model, vehicle_name, 'MadeBy',
                                 entity_to_id, relation_to_id, id_to_entity, top_k=5)
            display_results(results, onto)
    
    # –ü—Ä–∏–º–µ—Ä 2: –ù–∞–π—Ç–∏ –≤—Å–µ –∞–≤—Ç–æ–º–æ–±–∏–ª–∏ –æ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è
    print("\n" + "="*80)
    print("üìù –ü—Ä–∏–º–µ—Ä 2: –ö–∞–∫–∏–µ –∞–≤—Ç–æ–º–æ–±–∏–ª–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç BMW?")
    print("   –ó–∞–ø—Ä–æ—Å: (?, MadeBy, BMW)")
    
    bmw_name = None
    for name in entity_to_id.keys():
        if name.upper() == 'BMW':
            bmw_name = name
            break
    
    if bmw_name and 'MadeBy' in relation_to_id:
        results = predict_head(model, 'MadeBy', bmw_name,
                             entity_to_id, relation_to_id, id_to_entity, top_k=10)
        display_results(results, onto)
    else:
        print("   ‚ö†Ô∏è  BMW –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∏–Ω–¥–µ–∫—Å–µ")


def demonstrate_similarity_search(model, entity_to_id, id_to_entity, onto):
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø–æ–∏—Å–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π"""
    print("\n" + "="*80)
    print("–î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø 2: –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π")
    print("="*80)
    
    # –ò—â–µ–º Vehicle –≤ –∏–Ω–¥–µ–∫—Å–µ
    vehicle_name = None
    for name in entity_to_id.keys():
        if any(char.isdigit() for char in name) and len(name.split('_')) >= 3:
            vehicle_name = name
            break
    
    if vehicle_name:
        print(f"\nüìù –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –Ω–∞: {clean_name_for_display(vehicle_name)}")
        similar = find_similar_entities(model, vehicle_name, entity_to_id, id_to_entity, top_k=5)
        
        print(f"\n‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(similar)} –ø–æ—Ö–æ–∂–∏—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π:")
        for idx, (similar_name, similarity) in enumerate(similar, 1):
            print(f"   {idx}. {clean_name_for_display(similar_name)} (—Å—Ö–æ–¥—Å—Ç–≤–æ: {similarity:.4f})")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
    print("="*80)
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ò –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø PyKEEN EMBEDDINGS")
    print("="*80)
    
    if not HAS_PYKEEN:
        print("\n‚ùå PyKEEN –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω!")
        print("   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install pykeen")
        return
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–Ω—Ç–æ–ª–æ–≥–∏—é
    print("\nüìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –æ–Ω—Ç–æ–ª–æ–≥–∏–∏...")
    onto = get_ontology("file://cars_ontology.owl").load()
    print(f"   ‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π: {len(list(onto.Vehicle.instances()))}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    model, triples_factory = load_pykeen_model()
    if not model:
        print("\n‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å")
        print("   –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—ã –∑–∞–ø—É—Å—Ç–∏–ª–∏: python create_kg_embeddings_pykeen.py")
        return
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–∞–ø–ø–∏–Ω–≥–∏
    entity_to_id, relation_to_id, id_to_entity, id_to_relation = load_entity_mappings()
    
    # –ï—Å–ª–∏ –º–∞–ø–ø–∏–Ω–≥–∏ –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∏—Å—å, —Å–æ–∑–¥–∞–µ–º –∏–∑ triples_factory
    if not entity_to_id and triples_factory:
        print("\nüìã –°–æ–∑–¥–∞–Ω–∏–µ –º–∞–ø–ø–∏–Ω–≥–æ–≤ –∏–∑ triples_factory...")
        entity_to_id = {}
        relation_to_id = {}
        
        for idx, entity in enumerate(triples_factory.entity_to_id.keys()):
            entity_to_id[entity] = idx
        
        for idx, rel in enumerate(triples_factory.relation_to_id.keys()):
            relation_to_id[rel] = idx
        
        id_to_entity = {v: k for k, v in entity_to_id.items()}
        id_to_relation = {v: k for k, v in relation_to_id.items()}
        
        print(f"   ‚úÖ –°–æ–∑–¥–∞–Ω–æ –º–∞–ø–ø–∏–Ω–≥–æ–≤: {len(entity_to_id)} —Å—É—â–Ω–æ—Å—Ç–µ–π, {len(relation_to_id)} –æ—Ç–Ω–æ—à–µ–Ω–∏–π")
    
    if not entity_to_id:
        print("\n‚ö†Ô∏è  –ú–∞–ø–ø–∏–Ω–≥–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å –Ω–∞–ø—Ä—è–º—É—é")
        print("   (PyKEEN —Ö—Ä–∞–Ω–∏—Ç –º–∞–ø–ø–∏–Ω–≥–∏ –≤–Ω—É—Ç—Ä–∏ –º–æ–¥–µ–ª–∏)")
    
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è 1: Link Prediction
    demonstrate_link_prediction(model, entity_to_id, relation_to_id, id_to_entity, onto)
    
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è 2: Similarity Search
    demonstrate_similarity_search(model, entity_to_id, id_to_entity, onto)
    
    print("\n" + "="*80)
    print("–î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê")
    print("="*80)
    print("\nüí° –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–¥–µ:")
    print("   from pykeen.models import TransE")
    print("   model = TransE.from_pretrained('kg_embeddings_pykeen')")
    print("   scores = model.score_hrt(head_ids, relation_ids, tail_ids)")


if __name__ == "__main__":
    main()

