# -*- coding: utf-8 -*-
from __future__ import annotations

import argparse
import re
import html
import urllib.parse
import requests
import logging
from pathlib import Path
from bs4 import BeautifulSoup
from unidecode import unidecode
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

from rdflib import Graph, Namespace, URIRef, Literal
from rdflib.namespace import RDF, RDFS, OWL
from typing import Optional, Callable

# -----------------------------
# –õ–û–ì–ò
# -----------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
)
logger = logging.getLogger("hp-kg")

# -----------------------------
# –ü–ê–†–ê–ú–ï–¢–†–´
# -----------------------------
BASE_IRI = "http://www.semanticweb.org/ekaterinakulesova/ontologies/2025/0/harry_potter#"
BASE = "https://harrypotter.fandom.com/ru/wiki/"
API_ENDPOINT = "https://harrypotter.fandom.com/ru/api.php"

REQUEST_DELAY = 0.2
RELATION_DELAY = 0.05

OUT_FILE = "harrypotter_kg_ru.ttl"
CHECKPOINT_EVERY = 120
_save_counter = 0

# -----------------------------
# RDF –≥—Ä–∞—Ñ
# -----------------------------
g = Graph()
HP = Namespace(BASE_IRI)
HPO = Namespace(BASE_IRI)
g.bind("hp", HP)
g.bind("hpo", HPO)
g.bind("rdfs", RDFS)
g.bind("owl", OWL)

def qn(term) -> str:
    try:
        return g.namespace_manager.normalizeUri(term)
    except Exception:
        return str(term)


def load_existing_graph(path: str):
    if not path:
        return
    file_path = Path(path)
    if not file_path.exists():
        logger.info("–§–∞–π–ª %s –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π –≥—Ä–∞—Ñ", file_path)
        return
    try:
        g.parse(file_path, format="turtle")
        logger.info("–ó–∞–≥—Ä—É–∂–µ–Ω–æ %s —Ç—Ä–∏–ø–ª–µ—Ç–æ–≤ –∏–∑ %s", len(g), file_path)
    except Exception as exc:
        logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å %s: %s", file_path, exc)

classes = {
    "Thing": HPO.Thing,
    "Artifact": HPO.Artifact,
    "Character": HPO.Character,
    "Human": HPO.Human,
    "Muggle": HPO.Muggle,
    "Squib": HPO.Squib,
    "Wizard": HPO.Wizard,
    "Magical_creature": HPO.Magical_creature,
    "Centaur": HPO.Centaur,
    "Ghost": HPO.Ghost,
    "Giant": HPO.Giant,
    "Giant_spider": HPO.Giant_spider,
    "House": HPO.House,
    "House_elf": HPO.House_elf,
    "Mermaid": HPO.Mermaid,
    "Event": HPO.Event,
    "Big": HPO.Big,
    "Small": HPO.Small,
    "Location": HPO.Location,
    "Organization": HPO.Organization,
    "Potion": HPO.Potion,
    "Role": HPO.Role,
    "Spell": HPO.Spell,
}
for c, p in [
    (classes["Artifact"], classes["Thing"]),
    (classes["Character"], classes["Thing"]),
    (classes["Human"], classes["Character"]),
    (classes["Muggle"], classes["Human"]),
    (classes["Squib"], classes["Human"]),
    (classes["Wizard"], classes["Human"]),
    (classes["Magical_creature"], classes["Character"]),
    (classes["Centaur"], classes["Magical_creature"]),
    (classes["Ghost"], classes["Magical_creature"]),
    (classes["Giant"], classes["Magical_creature"]),
    (classes["Giant_spider"], classes["Magical_creature"]),
    (classes["House_elf"], classes["Magical_creature"]),
    (classes["Mermaid"], classes["Magical_creature"]),
    (classes["Event"], classes["Thing"]),
    (classes["Big"], classes["Event"]),
    (classes["Small"], classes["Event"]),
    (classes["House"], classes["Thing"]),
    (classes["Location"], classes["Thing"]),
    (classes["Organization"], classes["Thing"]),
    (classes["Potion"], classes["Thing"]),
    (classes["Role"], classes["Thing"]),
    (classes["Spell"], classes["Thing"]),
]:
    g.add((c, RDF.type, OWL.Class))
    g.add((c, RDFS.subClassOf, p))

obj_props = {
    "activeAt": HPO.activeAt,
    "artifactInvolvedIn": HPO.artifactInvolvedIn,
    "hasOwner": HPO.hasOwner,
    "hasCreator": HPO.hasCreator,
    "hasManufacturer": HPO.hasManufacturer,
    "friendWith": HPO.friendWith,
    "hasRole": HPO.hasRole,
    "memberOf": HPO.memberOf,
    "participatedIn": HPO.participatedIn,
    "relativeOf": HPO.relativeOf,
    "hasFather": HPO.hasFather,
    "hasMother": HPO.hasMother,
    "marriedWith": HPO.marriedWith,
    "romanceWith": HPO.romanceWith,
    "studiedAt": HPO.studiedAt,
    "takePartInEvent": HPO.takePartInEvent,
    "tookPlaceAt": HPO.tookPlaceAt,
    "hasSibling": HPO.hasSibling,
    "hasBrother": HPO.hasBrother,
    "hasSister": HPO.hasSister,
    "hasChild": HPO.hasChild,
    "hasSon": HPO.hasSon,
    "hasDaughter": HPO.hasDaughter,
    "hasUncle": HPO.hasUncle,
    "hasAunt": HPO.hasAunt,
    "hasNephew": HPO.hasNephew,
    "hasNiece": HPO.hasNiece,
    "hasGrandparent": HPO.hasGrandparent,
    "hasGrandchild": HPO.hasGrandchild,
    # —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–µ –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
    "siblingOf": HPO.siblingOf,
    "brotherOf": HPO.brotherOf,
    "sisterOf": HPO.sisterOf,
    "childOf": HPO.childOf,
    "sonOf": HPO.sonOf,
    "daughterOf": HPO.daughterOf,

    "godfatherOf": HPO.godfatherOf,
    "godsonOf": HPO.godsonOf,
    "cousinOf": HPO.cousinOf,
    "nephewOf": HPO.nephewOf,
    "nieceOf": HPO.nieceOf,
}

data_props = {
    "hasWand": HPO.hasWand,
    "hasPatronus": HPO.hasPatronus,
    "purpose": HPO.purpose,
    "hasType": HPO.hasType,
    "formula": HPO.formula,
    "effect": HPO.effect,
}

for p in data_props.values():
    g.add((p, RDF.type, OWL.DatatypeProperty))
for p in obj_props.values():
    g.add((p, RDF.type, OWL.ObjectProperty))
for p in ["godfatherOf", "godsonOf", "cousinOf", "nephewOf", "nieceOf", "hasManufacturer"]:
    g.add((obj_props[p], RDF.type, OWL.ObjectProperty))

session = requests.Session()
retry = Retry(
    total=4, backoff_factor=0.5,
    status_forcelist=[429, 500, 502, 503, 504],
    allowed_methods=["GET"], raise_on_status=False,
)
adapter = HTTPAdapter(max_retries=retry)
session.mount("https://", adapter)
session.mount("http://", adapter)

def http_get(url: str) -> BeautifulSoup | None:
    try:
        r = session.get(url, headers={"User-Agent": "hp-kg-populator/1.0"}, timeout=20)
        if r.status_code == 200:
            return BeautifulSoup(r.text, "html.parser")
        logger.warning("HTTP %s: %s", r.status_code, url)
    except requests.RequestException as e:
        logger.warning("–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ %s: %s", url, e)
    return None

def save_checkpoint(force=False):
    global _save_counter
    if not force and _save_counter < CHECKPOINT_EVERY:
        return
    g.serialize(destination=OUT_FILE, format="turtle")
    logger.info("–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ %s (—Ç—Ä–∏–ø–ª–µ—Ç–æ–≤: %s)", OUT_FILE, len(g))
    _save_counter = 0

def bump_counter(n=1):
    global _save_counter
    _save_counter += n
    save_checkpoint(False)

SKIP_TITLE_PATTERNS = [
    r"\(–ø–µ—Ä—Å–æ–Ω–∞–∂–∏\)$", r"\(–ø–µ—Ä—Å–æ–Ω–∞–∂\)$", r"\(–ø–µ—Ä—Å–æ–Ω–∞–∂–∏ —Ñ–∏–ª—å–º–∞\)$",
    r"^–°–ø–∏—Å–æ–∫($|[ \t])", r"^–ü–µ—Ä—Å–æ–Ω–∞–∂–∏($|[ \t])", r"^–ö–∞—Ç–µ–≥–æ—Ä–∏—è:",
]
def should_skip_title(title: str) -> bool:
    if not title:
        return True
    t = title.strip()
    for pat in SKIP_TITLE_PATTERNS:
        if re.search(pat, t, flags=re.IGNORECASE):
            return True
    return False

def slugify(label: str) -> str:
    txt = html.unescape(label).strip()
    ascii_txt = unidecode(txt)
    ascii_txt = re.sub(r"[\s/]+", "_", ascii_txt)
    ascii_txt = re.sub(r"[^A-Za-z0-9_\-]", "", ascii_txt)
    return ascii_txt or "entity"

def hp_entity(s: str) -> URIRef:
    return HP[s]

def add_labeled_instance(uri: URIRef, label_ru: str, rdf_type: URIRef):
    already = (uri, RDF.type, None) in g
    g.add((uri, RDF.type, rdf_type))
    g.add((uri, RDFS.label, Literal(label_ru, lang="ru")))
    if not already:
        logger.info("%s ‚Üê %s (%s)", qn(rdf_type), label_ru, qn(uri))
        bump_counter()

def fandom_url(title_ru: str) -> str:
    return urllib.parse.urljoin(BASE, urllib.parse.quote(title_ru.replace(" ", "_")))

def parse_categories(soup: BeautifulSoup) -> set[str]:
    cats = set()
    for a in soup.select('.page-header__categories a.category[href^="/ru/wiki/–ö–∞—Ç–µ–≥–æ—Ä–∏—è:"]'):
        t = (a.get("title") or a.get_text(strip=True) or "").strip()
        if t.startswith("–ö–∞—Ç–µ–≥–æ—Ä–∏—è:"):
            cats.add(t.replace("–ö–∞—Ç–µ–≥–æ—Ä–∏—è:", "").strip())
    for a in soup.select('#articleCategories a[href^="/ru/wiki/–ö–∞—Ç–µ–≥–æ—Ä–∏—è:"]'):
        t = (a.get("title") or a.get_text(strip=True) or "").strip()
        if t.startswith("–ö–∞—Ç–µ–≥–æ—Ä–∏—è:"):
            cats.add(t.replace("–ö–∞—Ç–µ–≥–æ—Ä–∏—è:", "").strip())
    return cats

def parse_infobox(soup: BeautifulSoup) -> dict:
    data = {}
    box = soup.select_one(".portable-infobox")
    if not box:
        return data
    for row in box.select(".pi-data"):
        label = row.select_one(".pi-data-label")
        value = row.select_one(".pi-data-value")
        if not label or not value:
            continue
        key = label.get_text(separator=" ", strip=True)
        text = value.get_text(separator=" ", strip=True)
        links = []
        for a in value.select("a[href]"):
            href = a.get("href") or ""
            title = a.get("title") or a.get_text(strip=True)
            if href.startswith("/ru/wiki/") and title and "/–ö–∞—Ç–µ–≥–æ—Ä–∏—è:" not in href:
                links.append(title)
        data[key] = {"text": text, "links": links}
    return data

def ensure_entity(title_ru: str, rdf_type: URIRef):
    uri = hp_entity(slugify(title_ru))
    add_labeled_instance(uri, title_ru, rdf_type)
    return uri

def link_by_titles(subject_uri: URIRef, prop: URIRef, titles: list[str], fallback_type: URIRef):
    for t in titles:
        if should_skip_title(t):
            continue
        obj = hp_entity(slugify(t))
        if (obj, RDF.type, None) not in g:
            add_labeled_instance(obj, t, fallback_type)
            ##time.sleep(RELATION_DELAY)
        g.add((subject_uri, prop, obj))


detect_type_cache: dict[str, Optional[URIRef]] = {}

def determine_type_for_title(title_ru: str) -> Optional[URIRef]:
    if not title_ru:
        return None
    if title_ru in detect_type_cache:
        return detect_type_cache[title_ru]
    obj = hp_entity(slugify(title_ru))
    for _, _, t in g.triples((obj, RDF.type, None)):
        if isinstance(t, URIRef):
            detect_type_cache[title_ru] = t
            return t

    url = fandom_url(title_ru)
    soup = http_get(url)
    ##time.sleep(REQUEST_DELAY)
    if not soup:
        detect_type_cache[title_ru] = None
        return None
    if not soup.select_one(".portable-infobox"):
        detect_type_cache[title_ru] = None
        return None
    info = parse_infobox(soup)
    cats = parse_categories(soup)
    page_text = " ".join(soup.stripped_strings)
    rdf_type = type_from_sources(info, cats, page_text)
    detect_type_cache[title_ru] = rdf_type
    return rdf_type


def link_people_analyze(subject_uri: URIRef, prop: URIRef, titles: list[str], fallback_type: URIRef):
    for t in titles:
        if should_skip_title(t):
            continue
        detected_type = determine_type_for_title(t)
        use_type = detected_type or fallback_type
        obj = hp_entity(slugify(t))
        if (obj, RDF.type, None) not in g:
            add_labeled_instance(obj, t, use_type)
            ##time.sleep(RELATION_DELAY)
        g.add((subject_uri, prop, obj))


# -----------------------------
# 3) –ú–∞–ø–ø–∏–Ω–≥ –ø–æ–ª–µ–π –∏–Ω—Ñ–æ–±–æ–∫—Å–∞ -> —Å–≤–æ–π—Å—Ç–≤–∞/–∫–ª–∞—Å—Å—ã
# -----------------------------

FIELD_MAP = {
    "–î–æ–º": ("memberOf", classes["House"]),
    "–û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è": ("memberOf", classes["Organization"]),
    "–ü—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç—å": ("memberOf", classes["Organization"]),

    "–ú–µ—Å—Ç–æ –æ–±—É—á–µ–Ω–∏—è": ("studiedAt", classes["Location"]),
    "–û–±—É—á–∞–ª—Å—è –≤": ("studiedAt", classes["Location"]),
    "–®–∫–æ–ª–∞": ("studiedAt", classes["Location"]),
    "–£—á–∏–ª—Å—è –≤": ("studiedAt", classes["Location"]),

    "–†–æ–¥ –∑–∞–Ω—è—Ç–∏–π": ("hasRole", classes["Role"]),
    "–ü—Ä–æ—Ñ–µ—Å—Å–∏—è": ("hasRole", classes["Role"]),
    "–î–æ–ª–∂–Ω–æ—Å—Ç—å": ("hasRole", classes["Role"]),
    "–°–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å": ("hasRole", classes["Role"]),

    "–°—É–ø—Ä—É–≥": ("marriedWith", classes["Character"]),
    "–°—É–ø—Ä—É–≥–∞": ("marriedWith", classes["Character"]),
    "–°—É–ø—Ä—É–≥(–∞)": ("marriedWith", classes["Character"]),
    "–û—Ç–µ—Ü": ("hasFather", classes["Character"]),
    "–ú–∞—Ç—å": ("hasMother", classes["Character"]),
    "–†–æ–¥–∏—Ç–µ–ª–∏": ("hasParent", classes["Character"]),
    "–î—Ä—É–∑—å—è": ("friendWith", classes["Character"]),
    "–õ—é–±–æ–≤–Ω—ã–π –∏–Ω—Ç–µ—Ä–µ—Å": ("romanceWith", classes["Character"]),
    "–†–æ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è": ("romanceWith", classes["Character"]),

    "–°–µ–º—å—è": ("family_from_infobox", classes["Character"]),
    "–ë—Ä–∞—Ç": ("hasBrother", classes["Character"]),
    "–°–µ—Å—Ç—Ä–∞": ("hasSister", classes["Character"]),
    "–ë—Ä–∞—Ç—å—è": ("hasSibling", classes["Character"]),
    "–°—ë—Å—Ç—Ä—ã": ("hasSibling", classes["Character"]),
    "–ë—Ä–∞—Ç—å—è –∏ —Å—ë—Å—Ç—Ä—ã": ("hasSibling", classes["Character"]),
    "–î–µ—Ç–∏": ("hasChild", classes["Character"]),
    "–°—ã–Ω": ("hasSon", classes["Character"]),
    "–î–æ—á—å": ("hasDaughter", classes["Character"]),
    "–°—ã–Ω–æ–≤—å—è": ("hasChild", classes["Character"]),
    "–î–æ—á–µ—Ä–∏": ("hasChild", classes["Character"]),
    "–î—è–¥—è": ("hasUncle", classes["Character"]),
    "–¢—ë—Ç—è": ("hasAunt", classes["Character"]),
    "–ü–ª–µ–º—è–Ω–Ω–∏–∫": ("hasNephew", classes["Character"]),
    "–ü–ª–µ–º—è–Ω–Ω–∏—Ü–∞": ("hasNiece", classes["Character"]),
    "–î–µ–¥—É—à–∫–∞": ("hasGrandparent", classes["Character"]),
    "–ë–∞–±—É—à–∫–∞": ("hasGrandparent", classes["Character"]),
    "–í–Ω—É–∫": ("hasGrandchild", classes["Character"]),
    "–í–Ω—É—á–∫–∞": ("hasGrandchild", classes["Character"]),

    "–í–∏–¥": ("type_hint", None),
    "–í–∏–¥(—ã)": ("type_hint", None),
    "–†–∞—Å–∞": ("type_hint", None),
    "–†–∞—Å–∞/–≤–∏–¥": ("type_hint", None),
    "–ü—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç—å –∫ –≤–∏–¥—É": ("type_hint", None),
    "–ü–æ–ª": ("sex_hint", None),

    "–ß–∏—Å—Ç–æ—Ç–∞ –∫—Ä–æ–≤–∏": ("blood_status_hint", None),
}

CATEGORY_TO_CLASS = {
    "–õ—é–¥–∏": classes["Human"],
    "–ú–∞–≥–∏": classes["Wizard"],
    "–ú–∞–≥–∏ –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É": classes["Wizard"],
    "–ú–∞–≥–≥–ª—ã": classes["Muggle"],
    "–°–∫–≤–∏–±—ã": classes["Squib"],
    "–ú–∞–≥–ª–æ—Ä–æ–∂–¥—ë–Ω–Ω—ã–µ –≤–æ–ª—à–µ–±–Ω–∏–∫–∏": classes["Wizard"],
    "–ß–∏—Å—Ç–æ–∫—Ä–æ–≤–Ω—ã–µ –≤–æ–ª—à–µ–±–Ω–∏–∫–∏": classes["Wizard"],
    "–ü–æ–ª—É–∫—Ä–æ–≤–∫–∏": classes["Wizard"],
    "–î–æ–º–æ–≤—ã–µ —ç–ª—å—Ñ—ã": classes["House_elf"],
    "–ü—Ä–∏–≤–∏–¥–µ–Ω–∏—è": classes["Ghost"],
    "–ö–µ–Ω—Ç–∞–≤—Ä—ã": classes["Centaur"],
    "–ê–∫—Ä–æ–º–∞–Ω—Ç—É–ª—ã": classes["Giant_spider"],
    "–í–µ–ª–∏–∫–∞–Ω—ã": classes["Giant"],
    "–†—É—Å–∞–ª–∫–∏": classes["Mermaid"],
}

CREATURE_KEYWORDS = {
    "–∫–µ–Ω—Ç–∞–≤—Ä": classes["Centaur"],
    "–ø—Ä–∏–≤–∏–¥–µ–Ω–∏–µ": classes["Ghost"],
    "–≥–∏–≥–∞–Ω—Ç": classes["Giant"],
    "–∞–∫—Ä–æ–º–∞–Ω—Ç—É–ª": classes["Giant_spider"],
    "–¥–æ–º–æ–≤–æ–π —ç–ª—å—Ñ": classes["House_elf"],
    "—Ä—É—Å–∞–ª–∫": classes["Mermaid"],  # —Å—Ç–µ–º
}

def has_word(txt: str, pattern: str) -> bool:
    try:
        return re.search(rf"(?iu)\b{pattern}\w*\b", txt) is not None
    except re.error:
        patt = re.escape(pattern)
        return re.search(rf"(?iu)\b{patt}\w*\b", txt) is not None

def choose_most_specific(types: list[URIRef]) -> URIRef:
    priority = [
        classes["Centaur"], classes["Ghost"], classes["Giant"], classes["Giant_spider"],
        classes["House_elf"], classes["Mermaid"],
        classes["Wizard"], classes["Muggle"], classes["Squib"],
        classes["Human"], classes["Character"],
    ]
    for cls in priority:
        if cls in types:
            return cls
    return classes["Character"]


def type_from_sources(info: dict, cats: set[str], page_text: str) -> URIRef:

    def classify_by_purity() -> Optional[URIRef]:
        purity = (info.get("–ß–∏—Å—Ç–æ—Ç–∞ –∫—Ä–æ–≤–∏", {}) or {}).get("text", "")
        purity = purity.lower().strip()
        if not purity:
            return None
        if has_word(purity, "—Å–∫–≤–∏–±"):
            return classes["Squib"]
        if has_word(purity, "–º–∞–≥–ª–æ—Ä–æ–∂–¥") or has_word(purity, "–ø–æ–ª—É–∫—Ä–æ–≤"):
            return classes["Wizard"]
        if has_word(purity, "–º–∞–≥–ª") and not has_word(purity, "–º–∞–≥–ª–æ—Ä–æ–∂–¥"):
            return classes["Muggle"]
        if any(has_word(purity, s) for s in ["—á–∏—Å—Ç–æ–∫—Ä–æ–≤–Ω—ã–π", "–≥—Ä—è–∑–Ω–æ–∫—Ä–æ–≤", "–∫—Ä–æ–≤—å"]):
            return classes["Wizard"]
        return None

    raw_kind = ""
    for k in ("–í–∏–¥", "–í–∏–¥(—ã)", "–†–∞—Å–∞", "–†–∞—Å–∞/–≤–∏–¥", "–ü—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç—å –∫ –≤–∏–¥—É"):
        if k in info:
            raw_kind = info[k]["text"].lower().strip()
            break

    if raw_kind:
        for key, cls in CREATURE_KEYWORDS.items():
            if key in raw_kind:
                return cls
        if "–ø—Ä–∏–≤–∏–¥–µ–Ω–∏–µ" in raw_kind or "–ø—Ä–∏–∑—Ä–∞–∫" in raw_kind:
            return classes["Ghost"]
        if "–≤–µ–ª–∏–∫–∞–Ω" in raw_kind:
            return classes["Giant"]
        if "–∞–∫—Ä–æ–º–∞–Ω—Ç—É–ª" in raw_kind:
            return classes["Giant_spider"]
        if "–¥–æ–º–æ–≤–æ–π —ç–ª—å—Ñ" in raw_kind or ("–¥–æ–º–æ–≤–æ–π" in raw_kind and "—ç–ª—å—Ñ" in raw_kind):
            return classes["House_elf"]
        if "—Ä—É—Å–∞–ª–∫" in raw_kind:
            return classes["Mermaid"]
        if "–≤–æ–ª—à–µ–±–Ω–∏–∫" in raw_kind or "–º–∞–≥" in raw_kind:
            return classes["Wizard"]
        if "—á–µ–ª–æ–≤–µ–∫" in raw_kind:
            by_purity = classify_by_purity()
            if by_purity:
                return by_purity
            if "–î–æ–º" in info:
                house = info["–î–æ–º"]["text"].lower()
                if any(h in house for h in ["–≥—Ä–∏—Ñ—Ñ–∏–Ω–¥–æ—Ä", "—Å–ª–∏–∑–µ—Ä–∏–Ω", "–∫–æ–≥—Ç–µ–≤—Ä–∞–Ω", "–ø—É—Ñ—Ñ–µ–Ω–¥—É–π"]):
                    return classes["Wizard"]
            if "–æ–±—É—á–∞–ª—Å—è" in page_text.lower() or "—Ö–æ–≥–≤–∞—Ä—Ç—Å" in page_text.lower() and "—É—á–∏–ª—Å—è" in page_text.lower():
                return classes["Wizard"]
            return classes["Human"]

    by_purity = classify_by_purity()
    if by_purity:
        return by_purity

    for c in cats:
        if c in CATEGORY_TO_CLASS:
            return CATEGORY_TO_CLASS[c]
        if "—Å–∫–≤–∏–±" in c.lower():
            return classes["Squib"]
        if "–º–∞–≥–≥–ª" in c.lower() or "–º—É–≥–ª" in c.lower():
            return classes["Muggle"]
        if any(x in c.lower() for x in ["—Ö–æ–≥–≤–∞—Ä—Ç—Å", "–º–∞–≥", "–≤–æ–ª—à–µ–±–Ω–∏–∫", "—É—á–µ–Ω–∏–∫", "–ø—Ä–æ—Ñ–µ—Å—Å–æ—Ä"]):
            pass

    txt = page_text.lower()

    muggle_phrases = [
        "–Ω–µ –º–∞–≥", "–Ω–µ–º–∞–≥", "–º–∞–≥–≥–ª", "–º—É–≥–ª", "muggle", "non-mag",
        "–Ω–µ –≤–æ–ª—à–µ–±–Ω–∏–∫", "–æ–±—ã—á–Ω—ã–π —á–µ–ª–æ–≤–µ–∫", "–ø—Ä–æ—Å—Ç–æ–π —á–µ–ª–æ–≤–µ–∫",
        "–Ω–µ –∏–º–µ–µ—Ç –º–∞–≥–∏—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π", "–Ω–µ –æ–±—É—á–∞–ª—Å—è –≤ —Ö–æ–≥–≤–∞—Ä—Ç—Å–µ",
        "–Ω–µ –≤–ª–∞–¥–µ–µ—Ç –º–∞–≥–∏–µ–π"
    ]
    for phrase in muggle_phrases:
        if phrase in txt:
            if "—Å–∫–≤–∏–±" in txt:
                return classes["Squib"]
            return classes["Muggle"]

    if ("—Ö–æ–≥–≤–∞—Ä—Ç—Å" in txt and ("—É—á–∏–ª—Å—è" in txt or "–ø–æ—Å–µ—â–∞–ª" in txt or "–≤—ã–ø—É—Å–∫–Ω–∏–∫" in txt)) \
       or "–ø–∞–ª–æ—á–∫–∞" in txt \
       or "–∑–∞–∫–ª–∏–Ω" in txt \
       or "–≤–æ–ª—à–µ–±–Ω–∏–∫" in txt \
       or "–º–∞–≥–∏" in txt \
       or "—á–∞—Ä—ã" in txt:
        return classes["Wizard"]

    for key, cls in CREATURE_KEYWORDS.items():
        if has_word(txt, key):
            return cls

    return classes["Human"]

def extract_family_relations_from_text(soup: BeautifulSoup, subject_title: str) -> list[tuple[str, str]]:
    relations = []

    content = soup.select_one(".mw-parser-output")
    if not content:
        return relations

    text_blocks = []
    for elem in content.children:
        if elem.name in {"p", "ul", "ol", "h2", "h3"}:
            text_blocks.append(elem.get_text(separator=" ", strip=True))

    full_text = " ".join(text_blocks)

    patterns = [
        ("hasBrother", r"(?:–±—Ä–∞—Ç(?:–∞|—É|–æ–º)?|–±—Ä–∞—Ç\s+–µ–≥–æ|–µ–≥–æ\s+–±—Ä–∞—Ç)\s+‚Äî?\s*([–ê-–Ø–Å][–∞-—è—ë\s\-]+?(?:\s[–ê-–Ø–Å]\.)?)"),
        ("hasSister", r"(?:—Å–µ—Å—Ç—Ä(?:–∞|—ã|—É|–æ–π)?|—Å–µ—Å—Ç—Ä–∞\s+–µ–≥–æ|–µ–≥–æ\s+—Å–µ—Å—Ç—Ä–∞)\s+‚Äî?\s*([–ê-–Ø–Å][–∞-—è—ë\s\-]+?(?:\s[–ê-–Ø–Å]\.)?)"),
        ("hasSon", r"(?:—Å—ã–Ω(?:–∞|—É|–æ–º)?|–µ–≥–æ\s+—Å—ã–Ω)\s+‚Äî?\s*([–ê-–Ø–Å][–∞-—è—ë\s\-]+?(?:\s[–ê-–Ø–Å]\.)?)"),
        ("hasDaughter", r"(?:–¥–æ—á—å|–¥–æ—á–µ—Ä(?:–∏|—å—é)?|–µ–≥–æ\s+–¥–æ—á—å)\s+‚Äî?\s*([–ê-–Ø–Å][–∞-—è—ë\s\-]+?(?:\s[–ê-–Ø–Å]\.)?)"),
        ("hasFather", r"(?:–æ—Ç–µ—Ü|–µ–≥–æ\s+–æ—Ç–µ—Ü)\s+‚Äî?\s*([–ê-–Ø–Å][–∞-—è—ë\s\-]+?(?:\s[–ê-–Ø–Å]\.)?)"),
        ("hasMother", r"(?:–º–∞—Ç—å|–µ–≥–æ\s+–º–∞—Ç—å)\s+‚Äî?\s*([–ê-–Ø–Å][–∞-—è—ë\s\-]+?(?:\s[–ê-–Ø–Å]\.)?)"),
        ("marriedWith", r"(?:—Å—É–ø—Ä—É–≥(?:–∞|–∏)?|–º—É–∂|–∂–µ–Ω–∞|–∂–µ–Ω–∞—Ç\s+–Ω–∞|–∑–∞–º—É–∂–µ–º\s+–∑–∞)\s+‚Äî?\s*([–ê-–Ø–Å][–∞-—è—ë\s\-]+?(?:\s[–ê-–Ø–Å]\.)?)"),
    ]

    for rel_type, pattern in patterns:
        for match in re.finditer(pattern, full_text, flags=re.IGNORECASE | re.UNICODE):
            name_candidate = match.group(1).strip()
            name_candidate = re.sub(r"[,\.!\?;:]+$", "", name_candidate)
            name_candidate = re.sub(r"\s+(?:–∏|–∏–ª–∏|—Ç–∞–∫–∂–µ|—Ä–∞–Ω–µ–µ|–ø–æ–∑–∂–µ)\s+.*$", "", name_candidate)
            if name_candidate and not should_skip_title(name_candidate):
                if len(name_candidate.split()) >= 2 and re.match(r"^[–ê-–Ø–Å][–∞-—è—ë]+(?:\s+[–ê-–Ø–Å][–∞-—è—ë]+)+$", name_candidate):
                    relations.append((rel_type, name_candidate))


    return relations


def parse_family_section(soup: BeautifulSoup) -> list[tuple[str, str]]:
    relations = []

    family_header = None
    for h2 in soup.select("h2"):
        if "—Å–µ–º—å—è" in h2.get_text(strip=True).lower():
            family_header = h2
            break

    if not family_header:
        return relations

    next_elem = family_header.find_next_sibling()
    family_text = ""
    while next_elem and next_elem.name != "h2":
        if next_elem.name in {"p", "ul", "ol"}:
            family_text += " " + next_elem.get_text(separator=" ", strip=True)
        next_elem = next_elem.find_next_sibling()

    if not family_text.strip():
        return relations

    items = [item.strip() for item in re.split(r"[,;]", family_text)]

    role_map = {
        "–æ—Ç–µ—Ü": "hasFather",
        "–º–∞—Ç—å": "hasMother",
        "–±—Ä–∞—Ç": "hasBrother",
        "—Å–µ—Å—Ç—Ä–∞": "hasSister",
        "—Å—ã–Ω": "hasSon",
        "–¥–æ—á—å": "hasDaughter",
        "–¥—è–¥—è": "hasUncle",
        "—Ç—ë—Ç—è": "hasAunt",
        "–ø–ª–µ–º—è–Ω–Ω–∏–∫": "hasNephew",
        "–ø–ª–µ–º—è–Ω–Ω–∏—Ü–∞": "hasNiece",
        "–¥–µ–¥—É—à–∫–∞": "hasGrandparent",
        "–±–∞–±—É—à–∫–∞": "hasGrandparent",
        "–≤–Ω—É–∫": "hasGrandchild",
        "–≤–Ω—É—á–∫–∞": "hasGrandchild",
        "–∫—É–∑–∏–Ω–∞": "cousinOf",
        "–∫—É–∑–µ–Ω": "cousinOf",
        "–¥–≤–æ—é—Ä–æ–¥–Ω–∞—è –ø–ª–µ–º—è–Ω–Ω–∏—Ü–∞": "nieceOf",
        "–¥–≤–æ—é—Ä–æ–¥–Ω—ã–π –ø–ª–µ–º—è–Ω–Ω–∏–∫": "nephewOf",
        "—Ç—Ä–æ—é—Ä–æ–¥–Ω—ã–π –ø–ª–µ–º—è–Ω–Ω–∏–∫": "nephewOf",
        "–∫—Ä–µ—Å—Ç–Ω–∏–∫": "godsonOf",
        "–∫—Ä—ë—Å—Ç–Ω—ã–π –æ—Ç–µ—Ü": "godfatherOf",
    }

    for item in items:
        item = re.sub(r"[‚Ä†*¬´¬ª\"]", "", item).strip()
        if not item:
            continue

        match = re.match(r"([–ê-–Ø–Å][–∞-—è—ë\s\-]+?)\s*\(([^)]+)\)", item)
        if not match:
            continue

        name = match.group(1).strip()
        role_desc = match.group(2).strip().lower()

        for role_name, prop_key in role_map.items():
            if role_name in role_desc:
                relations.append((prop_key, name))
                break

    return relations


def parse_family_field_from_infobox(family_text: str) -> list[tuple[str, str]]:
    logger.debug("üîç –†–∞–∑–±–æ—Ä –ø–æ–ª—è '–°–µ–º—å—è': %r", family_text[:200])
    relations = []

    cleaned_text = re.sub(r"[\‚Ä†*¬´¬ª\"]|\[\s*\d+\s*\]", "", family_text).strip()
    if not cleaned_text:
        return relations

    role_map = {
        "–æ—Ç–µ—Ü": "hasFather", "—Å–≤—ë–∫–æ—Ä": "hasFather", "—Ç–µ—Å—Ç—å": "hasFather",
        "–º–∞—Ç—å": "hasMother", "—Å–≤–µ–∫—Ä–æ–≤—å": "hasMother", "—Ç—ë—â–∞": "hasMother",
        "–±—Ä–∞—Ç": "hasBrother", "–¥–µ–≤–µ—Ä—å": "hasBrother", "—à—É—Ä–∏–Ω": "hasBrother",
        "—Å–µ—Å—Ç—Ä–∞": "hasSister", "–∑–æ–ª–æ–≤–∫–∞": "hasSister",
        "—Å—ã–Ω": "hasSon",
        "–¥–æ—á—å": "hasDaughter",
        "–¥—è–¥—è": "hasUncle",
        "—Ç—ë—Ç—è": "hasAunt",
        "–ø–ª–µ–º—è–Ω–Ω–∏–∫": "hasNephew", "–≤–Ω—É—á–∞—Ç—ã–π –ø–ª–µ–º—è–Ω–Ω–∏–∫": "hasNephew",
        "–ø–ª–µ–º—è–Ω–Ω–∏—Ü–∞": "hasNiece",
        "–¥–µ–¥": "hasGrandparent", "–¥–µ–¥—É—à–∫–∞": "hasGrandparent",
        "–±–∞–±—É—à–∫–∞": "hasGrandparent",
        "–≤–Ω—É–∫": "hasGrandchild",
        "–≤–Ω—É—á–∫–∞": "hasGrandchild",
        "–∂–µ–Ω–∞": "marriedWith", "—Å—É–ø—Ä—É–≥–∞": "marriedWith",
        "–º—É–∂": "marriedWith", "—Å—É–ø—Ä—É–≥": "marriedWith",
        "–∫—É–∑–∏–Ω–∞": "cousinOf", "–¥–≤–æ—é—Ä–æ–¥–Ω–∞—è —Å–µ—Å—Ç—Ä–∞": "cousinOf",
        "–∫—É–∑–µ–Ω": "cousinOf", "–¥–≤–æ—é—Ä–æ–¥–Ω—ã–π –±—Ä–∞—Ç": "cousinOf",
        "–∫—Ä–µ—Å—Ç–Ω–∏–∫": "godsonOf",
        "–∫—Ä—ë—Å—Ç–Ω—ã–π –æ—Ç–µ—Ü": "godfatherOf",
        "–∑—è—Ç—å": "sonOf",
        "–Ω–µ–≤–µ—Å—Ç–∫–∞": "daughterOf",
        "–ø—Ä–µ–¥–æ–∫": "relativeOf",
        "–ø–æ—Ç–æ–º–æ–∫": "relativeOf",
    }

    pattern = re.compile(r"([–ê-–Ø–Å][^()]+?)\s*\(([^)]+)\)")

    for match in pattern.finditer(cleaned_text):
        name = match.group(1).strip()
        role_desc = match.group(2).strip().lower()

        if len(name) < 2 or should_skip_title(name):
            continue

        found_role = False
        sorted_roles = sorted(role_map.keys(), key=len, reverse=True)
        for role_name in sorted_roles:
            if re.search(rf"\b{re.escape(role_name)}\b", role_desc):
                prop_key = role_map[role_name]
                relations.append((prop_key, name))
                found_role = True
                break

        if not found_role:
            for role_name, prop_key in role_map.items():
                if role_name in role_desc:
                    relations.append((prop_key, name))
                    break

    return relations

def scrape_character(title_ru: str):
    if should_skip_title(title_ru):
        return
    url = fandom_url(title_ru)
    soup = http_get(url)
    if not soup:
        logger.warning("–ü—Ä–æ–ø—É—Å–∫ (–Ω–µ—Ç –¥–æ—Å—Ç—É–ø–∞): %s", title_ru)
        return

    if not soup.select_one(".portable-infobox"):
        logger.debug("–ü—Ä–æ–ø—É—Å–∫ (–Ω–µ—Ç –∏–Ω—Ñ–æ–±–æ–∫—Å–∞): %s", title_ru)
        return

    info = parse_infobox(soup)
    cats = parse_categories(soup)
    rdf_type = type_from_sources(info, cats, soup.get_text(separator=" ", strip=True))
    subj = ensure_entity(title_ru, rdf_type)

    if "–ü–æ–ª" in info:
        g.add((subj, RDFS.comment, Literal(f"–ü–æ–ª: {info['–ü–æ–ª']['text']}", lang="ru")))

    direct_fields_map = {
        "–§–∞–∫—É–ª—å—Ç–µ—Ç": ("facultet", obj_props["memberOf"], "object"),
        "–ü–∞–ª–æ—á–∫–∞": ("wand", data_props["hasWand"], "data"),
        "–ü–∞—Ç—Ä–æ–Ω—É—Å": ("patronus", data_props["hasPatronus"], "data"),
        "–†–∞–±–æ—Ç–∞": ("job", obj_props["hasRole"], "role"),
    }

    for info_key, (log_key, prop_uri, prop_type) in direct_fields_map.items():
        if info_key in info:
            value_text = info[info_key].get("text", "").strip()
            if not value_text:
                continue

            if info_key == "–§–∞–∫—É–ª—å—Ç–µ—Ç" and rdf_type != classes["Wizard"]:
                continue

            if prop_type == "data":
                g.add((subj, prop_uri, Literal(value_text, lang="ru")))
            elif prop_type == "object":
                if value_text in HOUSES:
                    obj_uri = ensure_entity(value_text, classes["House"])
                    g.add((subj, prop_uri, obj_uri))
                else:
                    continue
            elif prop_type == "role":
                obj_uri = ensure_entity(value_text, classes["Role"])
                g.add((subj, prop_uri, obj_uri))

            logger.info(f"hpo:{log_key} {title_ru} -> {value_text}")
            bump_counter()

    logger.debug("–ò–Ω—Ñ–æ–±–æ–∫—Å –¥–ª—è %s: %s", title_ru, list(info.keys()))
    for key, val in info.items():
        if key not in FIELD_MAP:
            continue
        prop_key, fallback_cls = FIELD_MAP[key]

        if prop_key == "family_from_infobox":
            if val["text"]:
                family_rels = parse_family_field_from_infobox(val["text"])
                for rel_type, person_title in family_rels:
                    if rel_type in obj_props:
                        prop_uri = obj_props[rel_type]
                        detected_type = determine_type_for_title(person_title)
                        use_type = detected_type or classes["Character"]
                        obj = ensure_entity(person_title, use_type)
                        g.add((subj, prop_uri, obj))
                        logger.info("üë®‚Äçüë©‚Äçüëß‚Äçüë¶ –ò–Ω—Ñ–æ–±–æ–∫—Å-–°–µ–º—å—è: %s --%s--> %s", title_ru, rel_type, person_title)
                        bump_counter()
            continue

        if prop_key in ("type_hint", "sex_hint", "blood_status_hint") or prop_key not in obj_props:
            continue
        prop_uri = obj_props[prop_key]

        PERSON_RELATIONS = {"marriedWith", "hasFather", "hasMother", "friendWith",
                            "romanceWith", "relativeOf", "hasParent"}
        if prop_key in PERSON_RELATIONS:
            if prop_key == "hasParent" and val["links"]:
                for link_title in val["links"]:
                    if any(link_title.endswith(end) for end in ["–∞", "—è", "–∏—è", "–∏–Ω–∞", "—å–Ω–∞", "–Ω–∞"]):
                        g.add((subj, obj_props["hasMother"], hp_entity(slugify(link_title))))
                    else:
                        g.add((subj, obj_props["hasFather"], hp_entity(slugify(link_title))))
                    detected = determine_type_for_title(link_title)
                    use_type = detected or classes["Character"]
                    ensure_entity(link_title, use_type)
                    logger.info("üë®‚Äçüë¶ –†–æ–¥–∏—Ç–µ–ª—å: %s --%s--> %s", title_ru,
                                "hasMother" if "–∞" in link_title[-1] else "hasFather", link_title)
                continue
            if val["links"]:
                link_people_analyze(subj, prop_uri, val["links"], fallback_cls)
            else:
                v = val["text"]
                if not v or should_skip_title(v):
                    continue
                detected = determine_type_for_title(v)
                use_type = detected or fallback_cls
                obj = ensure_entity(v, use_type)
                g.add((subj, prop_uri, obj))
            continue

        if val["links"]:
            link_by_titles(subj, prop_uri, val["links"], fallback_cls)
        else:
            v = val["text"]
            if not v or should_skip_title(v):
                continue
            obj = ensure_entity(v, fallback_cls)
            g.add((subj, prop_uri, obj))

    family_section_rels = parse_family_section(soup)
    for prop_key, person_title in family_section_rels:
        if prop_key in obj_props:
            prop_uri = obj_props[prop_key]
            detected_type = determine_type_for_title(person_title)
            use_type = detected_type or classes["Character"]
            obj = ensure_entity(person_title, use_type)
            g.add((subj, prop_uri, obj))
            if prop_key == "hasFather":
                g.add((obj, obj_props["childOf"], subj))
            elif prop_key == "hasMother":
                g.add((obj, obj_props["childOf"], subj))
            elif prop_key == "hasBrother":
                g.add((obj, obj_props["brotherOf"], subj))
            elif prop_key == "hasSister":
                g.add((obj, obj_props["sisterOf"], subj))
            elif prop_key == "cousinOf":
                g.add((obj, obj_props["cousinOf"], subj))
            elif prop_key == "godsonOf":
                g.add((obj, obj_props["godfatherOf"], subj))
            elif prop_key == "godfatherOf":
                g.add((obj, obj_props["godsonOf"], subj))
            logger.info("–°–µ–º—å—è: %s --%s--> %s", title_ru, prop_key, person_title)
            bump_counter()

    family_rels = extract_family_relations_from_text(soup, title_ru)
    for rel_type, person_title in family_rels:
        if rel_type in obj_props:
            prop_uri = obj_props[rel_type]
            detected_type = determine_type_for_title(person_title)
            use_type = detected_type or classes["Character"]
            obj = hp_entity(slugify(person_title))
            if (obj, RDF.type, None) not in g:
                add_labeled_instance(obj, person_title, use_type)
            g.add((subj, prop_uri, obj))
            logger.info("üîó –¢–µ–∫—Å—Ç: %s --%s--> %s", title_ru, rel_type, person_title)
            bump_counter()


ARTIFACT_FIELD_MAP = {
    "–í–ª–∞–¥–µ–ª–µ—Ü": ("hasOwner", classes["Thing"]),
    "–í–ª–∞–¥–µ–ª—å—Ü—ã": ("hasOwner", classes["Thing"]),
    "–•–æ–∑—è–∏–Ω": ("hasOwner", classes["Thing"]),
    "–°–æ–∑–¥–∞—Ç–µ–ª—å": ("hasCreator", classes["Thing"]),
    "–°–æ–∑–¥–∞—Ç–µ–ª–∏": ("hasCreator", classes["Thing"]),
    "–ò–∑–æ–±—Ä–µ—Ç–∞—Ç–µ–ª—å": ("hasCreator", classes["Thing"]),
    "–ò–∑–æ–±—Ä–µ—Ç–∞—Ç–µ–ª–∏": ("hasCreator", classes["Thing"]),
    "–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å": ("hasManufacturer", classes["Thing"]),
    "–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–∏": ("hasManufacturer", classes["Thing"]),
    "–ò–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—å": ("hasManufacturer", classes["Thing"]),
    "–ò–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª–∏": ("hasManufacturer", classes["Thing"]),
    "–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ": ("purpose", None),
    "–ü—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ": ("purpose", None),
    "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ": ("purpose", None),
}


def scrape_artifact(title_ru: str) -> bool:
    if should_skip_title(title_ru):
        return False
    subj = ensure_entity(title_ru, classes["Artifact"])
    url = fandom_url(title_ru)
    soup = http_get(url)
    if not soup:
        logger.warning("–ü—Ä–æ–ø—É—Å–∫ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–∞ (–Ω–µ—Ç –¥–æ—Å—Ç—É–ø–∞): %s", title_ru)
        return True
    info = parse_infobox(soup)
    if not info:
        return True

    for key, val in info.items():
        if key not in ARTIFACT_FIELD_MAP:
            continue
        prop_key, fallback_cls = ARTIFACT_FIELD_MAP[key]
        if prop_key == "purpose":
            text = val.get("text", "").strip()
            if text:
                g.add((subj, data_props["purpose"], Literal(text, lang="ru")))
                logger.info("üéØ –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: %s -> %s", title_ru, text)
                bump_counter()
            continue
        if prop_key not in obj_props:
            continue
        prop_uri = obj_props[prop_key]
        fallback = fallback_cls or classes["Thing"]
        added = False
        if val["links"]:
            link_by_titles(subj, prop_uri, val["links"], fallback)
            added = True
        else:
            text = val.get("text", "").strip()
            if text and not should_skip_title(text):
                obj = ensure_entity(text, fallback)
                g.add((subj, prop_uri, obj))
                added = True
        if added:
            logger.info("üîß –ê—Ä—Ç–µ—Ñ–∞–∫—Ç: %s --%s--> %s", title_ru, prop_key, val.get("text", ""))
            bump_counter()
    return True


SPELL_FIELD_MAP = {
    "–¢–∏–ø": "hasType",
    "–í–∏–¥": "hasType",
    "–í–∏–¥ –º–∞–≥–∏–∏": "hasType",
    "–í–µ—Ç–≤—å –º–∞–≥–∏–∏": "hasType",
    "–ö–∞—Ç–µ–≥–æ—Ä–∏—è": "hasType",
    "–ò–Ω–∫–∞—Ä–Ω–∞—Ü–∏—è": "formula",
    "–§–æ—Ä–º—É–ª–∞": "formula",
    "–°–ª–æ–≤–∞": "formula",
    "–≠—Ñ—Ñ–µ–∫—Ç": "effect",
    "–í–æ–∑–¥–µ–π—Å—Ç–≤–∏–µ": "effect",
    "–û–ø–∏—Å–∞–Ω–∏–µ": "effect",
}


POTION_FIELD_MAP = {
    "–°–æ–∑–¥–∞—Ç–µ–ª—å": "hasCreator",
    "–°–æ–∑–¥–∞—Ç–µ–ª–∏": "hasCreator",
    "–ò–∑–æ–±—Ä–µ—Ç–∞—Ç–µ–ª—å": "hasCreator",
    "–ò–∑–æ–±—Ä–µ—Ç–∞—Ç–µ–ª—å–Ω–∏—Ü–∞": "hasCreator",
    "–ò–∑–æ–±—Ä–µ—Ç–∞—Ç–µ–ª–∏": "hasCreator",
    "–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å": "hasManufacturer",
    "–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–∏": "hasManufacturer",
    "–ò–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—å": "hasManufacturer",
    "–ò–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª–∏": "hasManufacturer",
    "–≠—Ñ—Ñ–µ–∫—Ç": "effect",
    "–í–æ–∑–¥–µ–π—Å—Ç–≤–∏–µ": "effect",
    "–û–ø–∏—Å–∞–Ω–∏–µ": "effect",
}


def scrape_spell(title_ru: str) -> bool:
    if should_skip_title(title_ru):
        return False
    subj = ensure_entity(title_ru, classes["Spell"])
    url = fandom_url(title_ru)
    soup = http_get(url)
    if not soup:
        logger.warning("–ü—Ä–æ–ø—É—Å–∫ –∑–∞–∫–ª–∏–Ω–∞–Ω–∏—è (–Ω–µ—Ç –¥–æ—Å—Ç—É–ø–∞): %s", title_ru)
        return True
    info = parse_infobox(soup)
    if not info:
        return True

    for key, val in info.items():
        prop_key = SPELL_FIELD_MAP.get(key)
        if not prop_key or prop_key not in data_props:
            continue
        text_parts = []
        if val.get("links"):
            text_parts.extend([t.strip() for t in val["links"] if t and not should_skip_title(t)])
        raw_text = val.get("text", "").strip()
        if raw_text:
            text_parts.append(raw_text)
        text_parts = [t for t in text_parts if t]
        if not text_parts:
            continue
        literal_text = "; ".join(dict.fromkeys(text_parts))
        g.add((subj, data_props[prop_key], Literal(literal_text, lang="ru")))
        logger.info("‚ú® –ó–∞–∫–ª–∏–Ω–∞–Ω–∏–µ: %s --%s--> %s", title_ru, prop_key, literal_text)
        bump_counter()
    return True


def scrape_potion(title_ru: str) -> bool:
    if should_skip_title(title_ru):
        return False
    subj = ensure_entity(title_ru, classes["Potion"])
    url = fandom_url(title_ru)
    soup = http_get(url)
    if not soup:
        logger.warning("–ü—Ä–æ–ø—É—Å–∫ –∑–µ–ª—å—è (–Ω–µ—Ç –¥–æ—Å—Ç—É–ø–∞): %s", title_ru)
        return True
    info = parse_infobox(soup)
    if not info:
        return True

    for key, val in info.items():
        prop_key = POTION_FIELD_MAP.get(key)
        if not prop_key:
            continue
        if prop_key in obj_props:
            fallback = classes["Thing"]
            added = False
            if val.get("links"):
                link_by_titles(subj, obj_props[prop_key], val["links"], fallback)
                added = True
            else:
                text = val.get("text", "").strip()
                if text and not should_skip_title(text):
                    obj = ensure_entity(text, fallback)
                    g.add((subj, obj_props[prop_key], obj))
                    added = True
            if added:
                logger.info("üß™ –ó–µ–ª—å–µ (obj): %s --%s--> %s", title_ru, prop_key, val.get("text", ""))
                bump_counter()
        elif prop_key in data_props:
            text = val.get("text", "").strip()
            if text:
                g.add((subj, data_props[prop_key], Literal(text, lang="ru")))
                logger.info("üß™ –ó–µ–ª—å–µ: %s --%s--> %s", title_ru, prop_key, text)
                bump_counter()
    return True


def scrape_single_page_as(label_ru: str, rdf_type: URIRef):
    if should_skip_title(label_ru):
        return
    ensure_entity(label_ru, rdf_type)

category_prefix_cache: dict[str, list[str]] = {}


def fetch_categories_by_prefix(prefix: str, limit: int = 20) -> list[str]:
    if not prefix:
        return []
    prefix = prefix.strip()
    if not prefix:
        return []
    if prefix in category_prefix_cache:
        return category_prefix_cache[prefix]

    params = {
        "action": "query",
        "format": "json",
        "list": "allcategories",
        "acprefix": prefix,
        "aclimit": limit,
        "acprop": "size",
    }
    try:
        resp = session.get(API_ENDPOINT, params=params, timeout=20)
        if resp.status_code != 200:
            logger.debug("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º %s (HTTP %s)", prefix, resp.status_code)
            category_prefix_cache[prefix] = []
            return []
        data = resp.json()
        cats = [c.get("*") for c in data.get("query", {}).get("allcategories", []) if c.get("*")]
        category_prefix_cache[prefix] = cats
        return cats
    except (requests.RequestException, ValueError) as exc:
        logger.debug("–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π –ø–æ –ø—Ä–µ—Ñ–∏–∫—Å—É %s: %s", prefix, exc)
        category_prefix_cache[prefix] = []
        return []


def _iter_category_members_api(category_title_ru: str, cap: int | None = None, include_subcategories: bool = False):
    seen_titles: set[str] = set()
    seen_categories: set[str] = set()
    queue = [_cleanup_category_title(category_title_ru)]
    total = 0

    headers = {"User-Agent": "hp-kg-populator/1.0"}

    while queue:
        current_cat = queue.pop(0)
        if not current_cat or current_cat in seen_categories:
            continue
        seen_categories.add(current_cat)

        cmcontinue = None
        while True:
            params = {
                "action": "query",
                "format": "json",
                "list": "categorymembers",
                "cmlimit": 500,
                "cmtitle": f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è:{current_cat}",
                "cmprop": "title|type|ids",
                "cmnamespace": "0|14",
            }
            if cmcontinue:
                params["cmcontinue"] = cmcontinue
            try:
                resp = session.get(API_ENDPOINT, params=params, headers=headers, timeout=20)
                if resp.status_code != 200:
                    logger.warning("–ö–∞—Ç–µ–≥–æ—Ä–∏—è API HTTP %s: %s", resp.status_code, current_cat)
                    return
                data = resp.json()
            except (requests.RequestException, ValueError) as exc:
                logger.warning("–û—à–∏–±–∫–∞ API –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ %s: %s", current_cat, exc)
                return

            members = data.get("query", {}).get("categorymembers", [])
            for member in members:
                title = (member.get("title") or "").strip()
                if not title:
                    continue
                ns = member.get("ns")
                if ns == 14:  # –∫–∞—Ç–µ–≥–æ—Ä–∏—è
                    if include_subcategories:
                        sub_title = _cleanup_category_title(title)
                        if sub_title and sub_title not in seen_categories:
                            queue.append(sub_title)
                    continue
                if should_skip_title(title) or title in seen_titles:
                    continue
                seen_titles.add(title)
                yield title
                total += 1
                if cap and total >= cap:
                    return

            cmcontinue = data.get("continue", {}).get("cmcontinue")
            if not cmcontinue:
                break


def _cleanup_category_title(title: str | None) -> str:
    if not title:
        return ""
    cleaned = title.strip()
    for prefix in ("–ö–∞—Ç–µ–≥–æ—Ä–∏—è:", "Category:"):
        if cleaned.startswith(prefix):
            cleaned = cleaned[len(prefix):]
    return cleaned.strip()


def _extract_category_name(href: str, fallback: str | None = None) -> Optional[str]:
    candidate = urllib.parse.unquote(href or "")
    marker_pos = -1
    for marker in ("/–ö–∞—Ç–µ–≥–æ—Ä–∏—è:", "/Category:"):
        marker_pos = candidate.find(marker)
        if marker_pos != -1:
            candidate = candidate[marker_pos + len(marker):]
            break
    else:
        candidate = fallback or ""
    candidate = candidate.replace("_", " ")
    candidate = _cleanup_category_title(candidate)
    return candidate or None
def _iter_category_members_html(category_title_ru: str, cap: int | None = None, include_subcategories: bool = False):
    seen_titles: set[str] = set()
    seen_categories: set[str] = set()
    queue = [_cleanup_category_title(category_title_ru)]
    total = 0

    while queue:
        current_cat = queue.pop(0)
        if not current_cat:
            continue
        if current_cat in seen_categories:
            continue
        seen_categories.add(current_cat)

        url = fandom_url("–ö–∞—Ç–µ–≥–æ—Ä–∏—è:" + current_cat)
        while url:
            soup = http_get(url)
            ##time.sleep(REQUEST_DELAY)
            if not soup:
                break
            for a in soup.select("a.category-page__member-link"):
                href = a.get("href") or ""
                title = (a.get("title") or a.get_text(strip=True) or "").strip()
                if not title:
                    continue
                if "/–ö–∞—Ç–µ–≥–æ—Ä–∏—è:" in href or "/Category:" in href:
                    if include_subcategories:
                        sub_title = _extract_category_name(href, title)
                        if sub_title and sub_title not in seen_categories:
                            queue.append(sub_title)
                    continue
                if should_skip_title(title) or title in seen_titles:
                    continue
                seen_titles.add(title)
                yield title
                total += 1
                if cap and total >= cap:
                    return
            next_a = soup.select_one('a.category-page__pagination-next')
            url = next_a["href"] if (next_a and next_a.get("href")) else None
            if url and url.startswith("/"):
                url = urllib.parse.urljoin(BASE, url)


def iter_category_members(category_title_ru: str, cap: int | None = None, include_subcategories: bool = False):
    yielded_any = False
    for title in _iter_category_members_api(category_title_ru, cap=cap, include_subcategories=include_subcategories):
        yielded_any = True
        yield title
    if yielded_any:
        return
    logger.info("API –Ω–µ –≤–µ—Ä–Ω—É–ª —ç–ª–µ–º–µ–Ω—Ç—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ %s, —Ä–µ–∑–µ—Ä–≤–Ω—ã–π HTML-–ø–∞—Ä—Å–∏–Ω–≥", category_title_ru)
    for title in _iter_category_members_html(category_title_ru, cap=cap, include_subcategories=include_subcategories):
        yield title

def scrape_category_characters(category_title_ru: str, cap: int, delay: float = 0.2):
    logger.info("–ö–∞—Ç–µ–≥–æ—Ä–∏—è –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π: %s (cap=%s)", category_title_ru, cap)
    for title in iter_category_members(category_title_ru, cap=cap):
        scrape_character(title)
        ##time.sleep(delay)

def scrape_category_entities(
    category_title_ru: str,
    rdf_type: URIRef,
    cap: int,
    delay: float = 0.1,
    include_subcategories: bool = False,
) -> int:
    logger.info(
        "–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π: %s ‚Üí %s (cap=%s, subcats=%s)",
        category_title_ru,
        qn(rdf_type),
        cap,
        include_subcategories,
    )
    produced = 0
    handler = ENTITY_TYPE_HANDLERS.get(rdf_type)
    for title in iter_category_members(
        category_title_ru, cap=cap, include_subcategories=include_subcategories
    ):
        if handler:
            if handler(title):
                produced += 1
        else:
            ensure_entity(title, rdf_type)
            produced += 1
        ##time.sleep(delay)
    return produced

CHAR_SEED = [
    "–ì–∞—Ä—Ä–∏ –ü–æ—Ç—Ç–µ—Ä","–ì–µ—Ä–º–∏–æ–Ω–∞ –ì—Ä–µ–π–Ω–¥–∂–µ—Ä","–†–æ–Ω –£–∏–∑–ª–∏","–ê–ª—å–±—É—Å –î–∞–º–±–ª–¥–æ—Ä","–°–µ–≤–µ—Ä—É—Å –°–Ω–µ–≥–≥",
    "–î—Ä–∞–∫–æ –ú–∞–ª—Ñ–æ–π","–†—É–±–µ—É—Å –•–∞–≥—Ä–∏–¥","–ú–∏–Ω–µ—Ä–≤–∞ –ú–∞–∫–≥–æ–Ω–∞–≥–∞–ª–ª","–°–∏—Ä–∏—É—Å –ë–ª—ç–∫","–õ–æ—Ä–¥ –í–æ–ª–∞–Ω-–¥–µ-–ú–æ—Ä—Ç",
]
MUGGLE_SEED = [
    "–í–µ—Ä–Ω–æ–Ω –î—É—Ä—Å–ª—å", "–ü–µ—Ç—É–Ω–∏—è –î—É—Ä—Å–ª—å", "–î–∞–¥–ª–∏ –î—É—Ä—Å–ª—å",
    "–ú–∞—Ä–¥–∂ –î—É—Ä—Å–ª—å",  "–§–ª–µ—Ç—á–µ—Ä", "–§—Ä–∞–Ω–∫ –ë—Ä–∞–π—Ç",
    "–¢–æ–º –†–µ–¥–¥–ª (—Å—Ç–∞—Ä—à–∏–π)", "–ú—ç—Ä–∏ –†–µ–¥–¥–ª"]
SQUIB_SEED = [
    "–ê—Ä–≥—É—Å –§–∏–ª—á",
    "–ê—Ä–∞–±–µ–ª–ª–∞ –§–∏–≥–≥",
]
HOUSES = ["–ì—Ä–∏—Ñ—Ñ–∏–Ω–¥–æ—Ä", "–°–ª–∏–∑–µ—Ä–∏–Ω", "–ö–æ–≥—Ç–µ–≤—Ä–∞–Ω", "–ü—É—Ñ—Ñ–µ–Ω–¥—É–π"]
ORGS = ["–û—Ä–¥–µ–Ω –§–µ–Ω–∏–∫—Å–∞", "–ü–æ–∂–∏—Ä–∞—Ç–µ–ª–∏ —Å–º–µ—Ä—Ç–∏", "–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –º–∞–≥–∏–∏"]
LOCATIONS = ["–•–æ–≥–≤–∞—Ä—Ç—Å", "–ö–æ—Å–æ–π –ø–µ—Ä–µ—É–ª–æ–∫", "–•–æ–≥—Å–º–∏–¥", "–ê–∑–∫–∞–±–∞–Ω"]

PERSON_CATS = [
    "–ü–µ—Ä—Å–æ–Ω–∞–∂–∏", "–õ—é–¥–∏", "–ú–∞–≥–∏", "–£—á–µ–Ω–∏–∫–∏ –•–æ–≥–≤–∞—Ä—Ç—Å–∞", "–ü—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª–∏ –•–æ–≥–≤–∞—Ä—Ç—Å–∞",
    "–î–æ–º–æ–≤—ã–µ —ç–ª—å—Ñ—ã", "–ü—Ä–∏–≤–∏–¥–µ–Ω–∏—è", "–ö–µ–Ω—Ç–∞–≤—Ä—ã", "–ê–∫—Ä–æ–º–∞–Ω—Ç—É–ª—ã", "–í–µ–ª–∏–∫–∞–Ω—ã", "–†—É—Å–∞–ª–∫–∏",
]

ENTITY_CATS = [
    {
        "names": ["–õ–æ–∫–∞—Ü–∏–∏", "–ú–µ—Å—Ç–∞", "–õ–æ–∫–∞—Ü–∏–∏ (–ì–∞—Ä—Ä–∏ –ü–æ—Ç—Ç–µ—Ä)"],
        "type": classes["Location"],
        "include_subcategories": True,
        "keywords": ["–õ–æ–∫–∞—Ü–∏", "–ú–µ—Å—Ç–∞", "–ì–æ—Ä–æ–¥", "–°–µ–ª–æ"],
    },
    {
        "names": ["–û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏"],
        "type": classes["Organization"],
        "include_subcategories": False,
    },
    {
        "names": ["–ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã", "–ü—Ä–µ–¥–º–µ—Ç—ã", "–ú–∞–≥–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥–º–µ—Ç—ã", "–í–æ–ª—à–µ–±–Ω—ã–µ –ø—Ä–µ–¥–º–µ—Ç—ã", "–ú–∞–≥–∏—á–µ—Å–∫–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã"],
        "type": classes["Artifact"],
        "include_subcategories": True,
        "keywords": ["–ê—Ä—Ç–µ—Ñ–∞–∫—Ç", "–ü—Ä–µ–¥–º–µ—Ç", "–ú–∞–≥–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥–º–µ—Ç—ã", "–í–æ–ª—à–µ–±–Ω—ã–µ –ø—Ä–µ–¥–º–µ—Ç—ã", "–ú–∞–≥–∏—á–µ—Å–∫–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã"],
    },
    {
        "names": ["–î–æ–ª–∂–Ω–æ—Å—Ç–∏"],
        "type": classes["Role"],
        "include_subcategories": False,
    },
]

ENTITY_TYPE_HANDLERS: dict[URIRef, callable] = {
    classes["Artifact"]: scrape_artifact,
    classes["Spell"]: scrape_spell,
    classes["Potion"]: scrape_potion,
}

def parse_cli_args():
    parser = argparse.ArgumentParser(
        description="–°–∫—Ä–∞–ø–∏—Ç ru.fandom –∏ –Ω–∞–ø–æ–ª–Ω—è–µ—Ç RDF-–æ–Ω—Ç–æ–ª–æ–≥–∏—é –ø–æ –ì–∞—Ä—Ä–∏ –ü–æ—Ç—Ç–µ—Ä—É."
    )
    parser.add_argument(
        "--output",
        default=OUT_FILE,
        help="–ü—É—Ç—å –∫ TTL-—Ñ–∞–π–ª—É –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é %(default)s)",
    )
    parser.add_argument(
        "--load",
        default=None,
        help="TTL-—Ñ–∞–π–ª –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏. "
             "–ï—Å–ª–∏ –Ω–µ –∑–∞–¥–∞–Ω –∏ --fresh –Ω–µ —É–∫–∞–∑–∞–Ω, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è --output (–µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç).",
    )
    parser.add_argument(
        "--fresh",
        action="store_true",
        help="–ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª –∏ —Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ —Å –Ω—É–ª—è.",
    )
    parser.add_argument(
        "--spell-cap",
        type=int,
        default=200,
        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–∫–ª–∏–Ω–∞–Ω–∏–π –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é %(default)s).",
    )
    parser.add_argument(
        "--potion-cap",
        type=int,
        default=200,
        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–µ–ª–∏–π –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é %(default)s).",
    )
    return parser.parse_args()


def main():
    args = parse_cli_args()
    global OUT_FILE
    OUT_FILE = args.output

    load_path = None
    if args.load:
        load_path = args.load
    elif not args.fresh:
        candidate = Path(args.output)
        if candidate.exists():
            load_path = args.output
    if load_path:
        load_existing_graph(load_path)

    for h in HOUSES: scrape_single_page_as(h, classes["House"])
    for o in ORGS:   scrape_single_page_as(o, classes["Organization"])
    for l in LOCATIONS: scrape_single_page_as(l, classes["Location"])

    for name in CHAR_SEED:
        scrape_character(name)
        # time.sleep(0.4)
    for name in MUGGLE_SEED:
        scrape_character(name)
    for name in SQUIB_SEED:
        scrape_character(name)

    for cat in PERSON_CATS:
        cap = 500 if cat in ("–õ—é–¥–∏", "–ü–µ—Ä—Å–æ–Ω–∞–∂–∏") else 200
        scrape_category_characters(cat, cap=cap, delay=0.15)

    for cfg in ENTITY_CATS:
        names = cfg["names"]
        rdf_type = cfg["type"]
        include_subcats = cfg.get("include_subcategories", False)
        cap = cfg.get("cap", 300)
        if isinstance(names, str):
            names = [names]

        candidate_names: list[str] = []
        seen_names: set[str] = set()
        for n in names:
            cleaned = _cleanup_category_title(n)
            if cleaned and cleaned not in seen_names:
                candidate_names.append(cleaned)
                seen_names.add(cleaned)

        for keyword in cfg.get("keywords", []) or []:
            for suggested in fetch_categories_by_prefix(keyword):
                cleaned = _cleanup_category_title(suggested)
                if cleaned and cleaned not in seen_names:
                    candidate_names.append(cleaned)
                    seen_names.add(cleaned)

        scraped = False
        for cat_name in candidate_names:
            produced = scrape_category_entities(
                cat_name,
                rdf_type,
                cap=cap,
                delay=0.1,
                include_subcategories=include_subcats,
            )
            if produced:
                scraped = True
                break
        if not scraped:
            logger.warning(
                "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Å—É—â–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π %s ‚Üí %s",
                ", ".join(candidate_names) or "(–Ω–µ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤)",
                qn(rdf_type),
            )

    scrape_category_entities(
        "–ó–∞–∫–ª–∏–Ω–∞–Ω–∏—è",
        classes["Spell"],
        cap=args.spell_cap,
        delay=0.1,
        include_subcategories=True,
    )
    scrape_category_entities(
        "–ó–µ–ª—å—è",
        classes["Potion"],
        cap=args.potion_cap,
        delay=0.1,
        include_subcategories=True,
    )

    save_checkpoint(force=True)
    logger.info("–ì–æ—Ç–æ–≤–æ. –¢—Ä–∏–ø–ª–µ—Ç–æ–≤ –≤ –≥—Ä–∞—Ñ–µ: %s", len(g))

if __name__ == "__main__":
    main()