{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T10:43:26.315731Z",
     "start_time": "2025-12-26T10:42:59.578598900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import rdflib\n",
    "import torch\n",
    "from pykeen.models import DistMult\n",
    "from pykeen.pipeline import pipeline\n",
    "from pykeen.triples import TriplesFactory\n",
    "from sklearn.cluster import KMeans\n",
    "from pykeen.evaluation import RankBasedEvaluator\n",
    "from sklearn.metrics import accuracy_score, adjusted_rand_score\n",
    "import plotly.express as px\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "os.environ['LOKY_MAX_CPU_COUNT'] = '8'\n",
    "\n",
    "try:\n",
    "    import torch_directml\n",
    "except ImportError:\n",
    "    torch_directml = None\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except ImportError:\n",
    "    plt = None\n",
    "\n",
    "try:\n",
    "    from sklearn.manifold import TSNE\n",
    "    from sklearn.decomposition import PCA\n",
    "except ImportError:\n",
    "    TSNE = None\n",
    "    PCA = None"
   ],
   "id": "a485b1638903ce8f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\python_stuff\\graphs\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T10:42:37.512126400Z",
     "start_time": "2025-12-26T10:42:37.089828300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_graph_embeddings(rdf_file_path: str, save_dir: str = \"genshin_embedding_results\"):\n",
    "    print(f\"Загрузка данных\")\n",
    "    g = rdflib.Graph()\n",
    "    g.parse(rdf_file_path, format=\"xml\")\n",
    "    print(f\"Загружено триплетов: {len(g)}\")\n",
    "    triples = []\n",
    "    entity_types = {}\n",
    "    for s, p, o in g:\n",
    "        s_str, p_str, o_str = str(s), str(p), str(o)\n",
    "        triples.append([s_str, p_str, o_str])\n",
    "        if p_str == \"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\":\n",
    "            entity_types.setdefault(s_str, set()).add(o_str)\n",
    "    triples = np.array(triples, dtype=str)\n",
    "\n",
    "    tf = TriplesFactory.from_labeled_triples(triples)\n",
    "    training, testing = tf.split([0.8, 0.2], random_state=42)\n",
    "    print(f\"Сущностей: {len(tf.entity_to_id)}, отношений: {len(tf.relation_to_id)}\")\n",
    "    print(\"Обучение\")\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    device = \"cpu\"\n",
    "    result = pipeline(\n",
    "        training=training,\n",
    "        testing=testing,\n",
    "        model=\"DistMult\",\n",
    "        model_kwargs=dict(embedding_dim=300),\n",
    "        training_kwargs=dict(num_epochs=300),\n",
    "        random_seed=42,\n",
    "        device=device,\n",
    "    )\n",
    "    print(\"Обучение завершено\")\n",
    "\n",
    "    result.save_to_directory(save_dir)\n",
    "    tf_path = os.path.join(save_dir, \"triples_factory.pkl\")\n",
    "    with open(tf_path, \"wb\") as f:\n",
    "        pickle.dump(tf, f)\n",
    "    types_path = os.path.join(save_dir, \"entity_types.pkl\")\n",
    "    with open(types_path, \"wb\") as f:\n",
    "        pickle.dump(entity_types, f)\n",
    "    training_path = os.path.join(save_dir, \"training.pkl\")\n",
    "    with open(training_path, \"wb\") as f:\n",
    "        pickle.dump(training, f)\n",
    "    testing_path = os.path.join(save_dir, \"testing.pkl\")\n",
    "    with open(testing_path, \"wb\") as f:\n",
    "        pickle.dump(testing, f)\n",
    "    return result.model, tf, entity_types, training, testing\n",
    "\n",
    "\n",
    "def load_saved_model(directory: str = \"genshin_embedding_results\"):\n",
    "    from pykeen.models import DistMult as DistMultModel\n",
    "    model = DistMultModel.load(os.path.join(directory, \"model.pkl\"))\n",
    "    tf_path = os.path.join(directory, \"triples_factory.pkl\")\n",
    "    with open(tf_path, \"rb\") as f:\n",
    "        tf = pickle.load(f)\n",
    "    types_path = os.path.join(directory, \"entity_types.pkl\")\n",
    "    entity_types = {}\n",
    "    if os.path.exists(types_path):\n",
    "        with open(types_path, \"rb\") as f:\n",
    "            entity_types = pickle.load(f)\n",
    "    training_path = os.path.join(directory, \"training.pkl\")\n",
    "    with open(training_path, \"rb\") as f:\n",
    "        training = pickle.load(f)\n",
    "    testing_path = os.path.join(directory, \"testing.pkl\")\n",
    "    with open(testing_path, \"rb\") as f:\n",
    "        testing = pickle.load(f)\n",
    "    return model, tf, entity_types, training, testing\n",
    "\n",
    "\n",
    "def get_entity_embedding(entity_uri: str, model, tf):\n",
    "    try:\n",
    "        entity_id = tf.entity_to_id[entity_uri]\n",
    "        embedding = model.entity_representations[0](indices=torch.tensor([entity_id])).detach().numpy()[0]\n",
    "        return embedding\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_relation_embedding(relation_uri: str, model, tf):\n",
    "    try:\n",
    "        relation_id = tf.relation_to_id[relation_uri]\n",
    "        embedding = model.relation_representations[0](indices=torch.tensor([relation_id])).detach().numpy()[0]\n",
    "        return embedding\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def find_similar_entities(entity_uri: str, model, tf, entity_types: dict | None = None, top_k: int = 5):\n",
    "    entity_embedding = get_entity_embedding(entity_uri, model, tf)\n",
    "    if entity_embedding is None:\n",
    "        return []\n",
    "    all_entity_ids = torch.arange(len(tf.entity_to_id))\n",
    "    all_embeddings = model.entity_representations[0](indices=all_entity_ids).detach().numpy()\n",
    "    similarities = cosine_similarity([entity_embedding], all_embeddings)[0]\n",
    "    top_indices = np.argsort(similarities)[::-1][1:]\n",
    "    results = []\n",
    "    id_to_entity = {v: k for k, v in tf.entity_to_id.items()}\n",
    "    filtered = []\n",
    "    if entity_types and entity_uri in entity_types:\n",
    "        target_types = entity_types.get(entity_uri, set())\n",
    "        for idx in top_indices:\n",
    "            uri = id_to_entity[idx]\n",
    "            types = entity_types.get(uri, set())\n",
    "            if target_types & types:\n",
    "                filtered.append(idx)\n",
    "            if len(filtered) >= top_k:\n",
    "                break\n",
    "    else:\n",
    "        filtered = list(top_indices[:top_k])\n",
    "    i = 0\n",
    "    while len(filtered) < top_k and i < len(top_indices):\n",
    "        idx = top_indices[i]\n",
    "        if idx not in filtered:\n",
    "            filtered.append(idx)\n",
    "        i += 1\n",
    "    for idx in filtered[:top_k]:\n",
    "        similar_entity = id_to_entity[idx]\n",
    "        similarity = similarities[idx]\n",
    "        results.append((similar_entity, similarity))\n",
    "    return results\n",
    "\n",
    "\n",
    "def reduce_embeddings(embeddings, method: str = \"tsne\", random_state: int = 42, perplexity: int = 30):\n",
    "    if method == \"pca\":\n",
    "        if PCA is None:\n",
    "            raise ImportError(\"Скрипт запущен без scikit-learn. Установи scikit-learn для PCA.\")\n",
    "        reducer = PCA(n_components=2, random_state=random_state)\n",
    "    else:\n",
    "        if TSNE is None:\n",
    "            raise ImportError(\"Скрипт запущен без scikit-learn. Установи scikit-learn для t-SNE.\")\n",
    "        n_samples = len(embeddings)\n",
    "        if n_samples < 2:\n",
    "            raise ValueError(\"Слишком мало точек для t-SNE: нужно хотя бы 2 сущности.\")\n",
    "        safe_perplexity = min(perplexity, n_samples - 1)\n",
    "        reducer = TSNE(\n",
    "            n_components=2,\n",
    "            random_state=random_state,\n",
    "            perplexity=safe_perplexity,\n",
    "            init=\"pca\",\n",
    "            learning_rate=\"auto\",\n",
    "        )\n",
    "\n",
    "    return reducer.fit_transform(embeddings)\n",
    "\n",
    "\n",
    "def collect_all_embeddings(model, tf):\n",
    "    all_entity_ids = torch.arange(len(tf.entity_to_id))\n",
    "    embeddings = model.entity_representations[0](indices=all_entity_ids).detach().numpy()\n",
    "    id_to_entity = {v: k for k, v in tf.entity_to_id.items()}\n",
    "    labels = [id_to_entity[i] for i in range(len(embeddings))]\n",
    "    return embeddings, labels\n",
    "\n",
    "\n",
    "def plot_embeddings_2d(points_2d, labels, title: str = \"Embedding map\", max_labels: int = 75, save_path: str | None = None, figsize=(12, 10), cluster_labels=None):\n",
    "    plt.figure(figsize=figsize)\n",
    "    if cluster_labels is not None:\n",
    "        unique_clusters = len(np.unique(cluster_labels))\n",
    "        cmap = plt.cm.get_cmap('viridis', unique_clusters)\n",
    "        scatter = plt.scatter(points_2d[:, 0], points_2d[:, 1], s=12, alpha=0.6, c=cluster_labels, cmap=cmap)\n",
    "        plt.colorbar(scatter, label='Cluster')\n",
    "    else:\n",
    "        plt.scatter(points_2d[:, 0], points_2d[:, 1], s=12, alpha=0.6)\n",
    "    def _shorten(uri: str) -> str:\n",
    "        if \"#\" in uri:\n",
    "            return uri.split(\"#\")[-1]\n",
    "        return uri\n",
    "\n",
    "    for i, label in enumerate(labels[:max_labels]):\n",
    "        short = _shorten(label)\n",
    "        plt.annotate(short, (points_2d[i, 0], points_2d[i, 1]), fontsize=8, alpha=0.7)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=200)\n",
    "        print(f\"График сохранён в {save_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def visualize_embeddings(model: DistMult, triples_factory: TriplesFactory, method: str = \"tsne\", max_labels: int = 75, save_path: str = \"genshin_embedding_results/embeddings.png\", html_path: str = \"genshin_embedding_results/interactive_embeddings.html\", perplexity: int = 30, max_points: int = 1200, random_state: int = 42, num_clusters: int | None = None):\n",
    "    embeddings, labels = collect_all_embeddings(model, triples_factory)\n",
    "    if max_points and len(embeddings) > max_points:\n",
    "        rng = np.random.default_rng(random_state)\n",
    "        indices = rng.choice(len(embeddings), size=max_points, replace=False)\n",
    "        embeddings = embeddings[indices]\n",
    "        labels = [labels[i] for i in indices]\n",
    "        print(f\"Всего сущностей: {len(triples_factory.entity_to_id)}\")\n",
    "    else:\n",
    "        print(f\"Всего сущностей: {len(labels)}.\")\n",
    "    points_2d = reduce_embeddings(embeddings, method=method, perplexity=perplexity)\n",
    "\n",
    "    cluster_labels = None\n",
    "    if num_clusters:\n",
    "        print(f\"Кластеризация с {num_clusters} кластерами\")\n",
    "        kmeans = KMeans(n_clusters=num_clusters, random_state=random_state)\n",
    "        cluster_labels = kmeans.fit_predict(points_2d)\n",
    "\n",
    "    plot_embeddings_2d(\n",
    "        points_2d,\n",
    "        labels,\n",
    "        title=f\"Genshin Ontology Embeddings\",\n",
    "        max_labels=max_labels,\n",
    "        save_path=save_path,\n",
    "        cluster_labels=cluster_labels,\n",
    "    )\n",
    "\n",
    "    def _shorten(uri: str) -> str:\n",
    "        if \"#\" in uri:\n",
    "            return uri.split(\"#\")[-1]\n",
    "        return uri\n",
    "\n",
    "    short_labels = [_shorten(label) for label in labels]\n",
    "    fig = px.scatter(\n",
    "        x=points_2d[:, 0],\n",
    "        y=points_2d[:, 1],\n",
    "        color=cluster_labels,\n",
    "        hover_name=short_labels,\n",
    "        title=f\"Interactive Genshin Ontology Embeddings\",\n",
    "        labels={'color': 'Cluster' if cluster_labels is not None else None}\n",
    "    )\n",
    "\n",
    "    fig.update_traces(marker=dict(size=12, opacity=0.6))\n",
    "    fig.write_html(html_path)\n",
    "\n",
    "\n",
    "def find_rdf_file():\n",
    "    rdf_path = \"data/genshin_ontology_clean.rdf\"\n",
    "    if os.path.exists(rdf_path):\n",
    "        return rdf_path\n",
    "    for file in os.listdir(\".\"):\n",
    "        if file.endswith(\".rdf\"):\n",
    "            return file\n",
    "    raise FileNotFoundError(\"RDF файл не найден\")\n",
    "\n",
    "\n",
    "def evaluate_model(model, training, testing):\n",
    "    evaluator = RankBasedEvaluator()\n",
    "    metrics = evaluator.evaluate(model, mapped_triples=testing.mapped_triples, additional_filter_triples=training.mapped_triples)\n",
    "\n",
    "    mr = metrics.get_metric('mean_rank')\n",
    "    mrr = metrics.get_metric('mean_reciprocal_rank')\n",
    "    hits_1 = metrics.get_metric('hits@1')\n",
    "    hits_3 = metrics.get_metric('hits@3')\n",
    "    hits_10 = metrics.get_metric('hits@10')\n",
    "\n",
    "    print(f\"MR: {mr:.4f}\")\n",
    "    print(f\"MRR: {mrr:.4f}\")\n",
    "    print(f\"Hits@1: {hits_1:.4f}, \\nHits@3: {hits_3:.4f}, \\nHits@10: {hits_10:.4f}\")\n",
    "\n",
    "\n",
    "def perform_clustering(model, tf, entity_types, num_clusters=5, random_state=42):\n",
    "    embeddings, labels = collect_all_embeddings(model, tf)\n",
    "    true_labels = []\n",
    "    type_to_id = {t: i for i, t in enumerate(set.union(*entity_types.values()))} if entity_types else {}\n",
    "    for label in labels:\n",
    "        types = entity_types.get(label, set())\n",
    "        true_label = list(types)[0] if types else 'Unknown'\n",
    "        true_labels.append(type_to_id.get(true_label, -1))\n",
    "    points_2d = reduce_embeddings(embeddings, method=\"tsne\")\n",
    "    plot_embeddings_2d(points_2d, labels, title=\"Expected Clusters (by Type)\", cluster_labels=np.array(true_labels))\n",
    "\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=random_state)\n",
    "    cluster_labels = kmeans.fit_predict(points_2d)\n",
    "    plot_embeddings_2d(points_2d, labels, title=\"KMeans Clusters\", cluster_labels=cluster_labels)\n",
    "\n",
    "    ari = adjusted_rand_score(true_labels, cluster_labels)\n",
    "    print(f\"ARI: {ari:.4f}\")"
   ],
   "id": "9c6ef131120d6b5",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DistMult' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 179\u001B[39m\n\u001B[32m    175\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    176\u001B[39m         plt.show()\n\u001B[32m--> \u001B[39m\u001B[32m179\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mvisualize_embeddings\u001B[39m(model: \u001B[43mDistMult\u001B[49m, triples_factory: TriplesFactory, method: \u001B[38;5;28mstr\u001B[39m = \u001B[33m\"\u001B[39m\u001B[33mtsne\u001B[39m\u001B[33m\"\u001B[39m, max_labels: \u001B[38;5;28mint\u001B[39m = \u001B[32m75\u001B[39m, save_path: \u001B[38;5;28mstr\u001B[39m = \u001B[33m\"\u001B[39m\u001B[33mgenshin_embedding_results/embeddings.png\u001B[39m\u001B[33m\"\u001B[39m, html_path: \u001B[38;5;28mstr\u001B[39m = \u001B[33m\"\u001B[39m\u001B[33mgenshin_embedding_results/interactive_embeddings.html\u001B[39m\u001B[33m\"\u001B[39m, perplexity: \u001B[38;5;28mint\u001B[39m = \u001B[32m30\u001B[39m, max_points: \u001B[38;5;28mint\u001B[39m = \u001B[32m1200\u001B[39m, random_state: \u001B[38;5;28mint\u001B[39m = \u001B[32m42\u001B[39m, num_clusters: \u001B[38;5;28mint\u001B[39m | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m    180\u001B[39m     embeddings, labels = collect_all_embeddings(model, triples_factory)\n\u001B[32m    181\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m max_points \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(embeddings) > max_points:\n",
      "\u001B[31mNameError\u001B[39m: name 'DistMult' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def main():\n",
    "    save_dir = \"genshin_embedding_results\"\n",
    "\n",
    "    try:\n",
    "        rdf_file = find_rdf_file()\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Ошибка: {e}\")\n",
    "        return\n",
    "\n",
    "    if os.path.exists(save_dir) and os.path.exists(os.path.join(save_dir, \"model.pkl\")):\n",
    "        model, tf, entity_types, training, testing = load_saved_model(save_dir)\n",
    "    else:\n",
    "        model, tf, entity_types, training, testing = train_graph_embeddings(rdf_file, save_dir)\n",
    "\n",
    "    similar = find_similar_entities(\"http://example.org/ontology/kirara\", model, tf, entity_types=entity_types, top_k=3)\n",
    "    if similar:\n",
    "        print(\" Похожие сущности:\")\n",
    "        for sim_entity, similarity in similar:\n",
    "            sim_name = sim_entity.split(\"/\")[-1]\n",
    "            print(f\" • {sim_name}: {similarity:.4f}\")\n",
    "    similar = find_similar_entities(\"http://example.org/ontology/hilichurl\", model, tf, entity_types=entity_types,\n",
    "                                    top_k=3)\n",
    "    if similar:\n",
    "        print(\" Похожие сущности:\")\n",
    "        for sim_entity, similarity in similar:\n",
    "            sim_name = sim_entity.split(\"/\")[-1]\n",
    "            print(f\" • {sim_name}: {similarity:.4f}\")\n",
    "    similar = find_similar_entities(\"http://example.org/ontology/favonius_sword\", model, tf, entity_types=entity_types,\n",
    "                                    top_k=3)\n",
    "    if similar:\n",
    "        print(\" Похожие сущности:\")\n",
    "        for sim_entity, similarity in similar:\n",
    "            sim_name = sim_entity.split(\"/\")[-1]\n",
    "            print(f\" • {sim_name}: {similarity:.4f}\")\n",
    "\n",
    "    visualize_embeddings(model, tf, method=\"tsne\", max_labels=75, save_path=\"genshin_embedding_results/embeddings.png\")\n",
    "\n",
    "    evaluate_model(model, training, testing)\n",
    "\n",
    "    perform_clustering(model, tf, entity_types, num_clusters=5)"
   ],
   "id": "8e616f250ed42c20"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "main()",
   "id": "8881881c8779ff51"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
