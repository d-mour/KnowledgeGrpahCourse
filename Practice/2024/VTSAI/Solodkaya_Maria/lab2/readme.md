# Лабораторная работа 2

Выполнили:
* Полежаева Евгения (P4240)
* Солодкая Мария (P4240)

### Стратегии обучения RL

В контексте обучения с подкреплением (Reinforcement Learning, RL), стратегии обучения модели определяют, каким образом агент исследует окружение и выбирает действия для максимизации награды. Эти стратегии разделяются на Exploration (исследование) и Exploitation (использование) и направлены на достижение баланса между изучением новых стратегий и максимизацией текущих знаний.
В обучении с подкреплением присутствует баланс между Exploration и Exploitation. Exploration включает в себя стратегии, направленные на изучение новых действий или состояний, чтобы расширить базу знаний. Exploitation, наоборот, использует текущие знания для выбора оптимальных действий и максимизации награды. Нахождение оптимального баланса между Exploration и Exploitation - ключевой аспект в достижении успешных стратегий обучения с подкреплением.
В данной работе мы рассмотрим различные стратегии и их влияние на обучение агента с использованием библиотек Gymnasium и Stable-Baseline3.

## Выбор среды

В данной работе мы использовали среду [MountainCarContinuous](https://www.gymlibrary.dev/environments/classic_control/mountain_car_continuous/), предоставляемую Gym. Эта задача заключается в управлении автомобилем. 

### Характеристика среды:
* MountainCarContinuous MDP - это детерминированный MDP, который состоит из автомобиля, стохастически размещенного на дне синусоидальной долины, при этом единственными возможными действиями являются ускорения, которые могут быть применены к автомобилю в любом направлении. Цель MDP - стратегически ускорить автомобиль, чтобы достичь целевого состояния на вершине нужного холма.
* Действие представляет собой массив данных с формой (1,), представляющий направленную силу, приложенную к автомобилю. Действие обрезается в диапазоне [-1,1] и умножается на степень 0,0015.
* Положению автомобиля присваивается равномерное случайное значение в [-0.6 , -0.4]. Начальной скорости автомобиля всегда присваивается значение 0.

## Выводы:

1. Сравнительный анализ двух алгоритмов обучения с подкреплением (SAC и DDPG) показал следующее:

* алгоритм мягких критических факторов (SAC) хорошо справляется с балансом между исследованиями и использованием.
* алгоритм SAC помогаат бороться с проблемой чрезмерного доверия к действиям агента.
* алгоритм DDPG подходит для задач с непрерывными пространствами действий.
* на производительность алгоритма глубкоих детерминированных градиентов политики (DDPG) может существенно повлиять выбор гиперпараметров (например, скорость обучения).

2. Изучены стратегии разведки и эксплуатации.
