# Лабораторная работа 2

Выполнили:
* Полежаева Евгения (P4240)
* Солодкая Мария (P4240)

### Стратегии обучения RL

В контексте обучения с подкреплением (Reinforcement Learning, RL), стратегии обучения модели определяют, каким образом агент исследует окружение и выбирает действия для максимизации награды. Эти стратегии разделяются на Exploration (исследование) и Exploitation (использование) и направлены на достижение баланса между изучением новых стратегий и максимизацией текущих знаний.
В обучении с подкреплением присутствует баланс между Exploration и Exploitation. Exploration включает в себя стратегии, направленные на изучение новых действий или состояний, чтобы расширить базу знаний. Exploitation, наоборот, использует текущие знания для выбора оптимальных действий и максимизации награды. Нахождение оптимального баланса между Exploration и Exploitation - ключевой аспект в достижении успешных стратегий обучения с подкреплением.
В данной работе мы рассмотрим различные стратегии и их влияние на обучение агента с использованием библиотек Gymnasium и Stable-Baseline3.

## Выбор среды

В данной работе мы будем использовать среду [Car Racing](https://www.gymlibrary.dev/environments/box2d/car_racing/), предоставляемую Gym. Эта задача заключается в управлении гоночной средой. 

### Характеристика среды:
* Генерируемая трасса является случайной в каждом эпизоде.
* При непрерывном режиме: есть 3 действия: рулевое управление (-1 полностью влево, + 1 полностью вправо), газ и отрыв. Если дискретно: есть 5 действий: ничего не делать, повернуть налево, повернуть направо, газ, тормоз.
* Состояние состоит из 96x96 пикселей.
* Машина трогается с места в центре дороги.
* Эпизод заканчивается, когда пройдены все плитки. Машина также может выехать за пределы игрового поля, то есть далеко за пределы трассы, и в этом случае она получит награду -100 и погибнет.

**Параметры:**

* `lap_complete_percent` определяет процентное количество плиток, которые должен просмотреть агент, прежде чем круг будет считаться завершенным.

* Прохождение `domain_randomize=True` включает рандомизированный вариант окружения в домене. В этом сценарии цвета фона и трассы различаются при каждом сбросе.

* Прохождение `continuous=False` преобразует окружающую среду для использования пространства дискретных действий. Пространство дискретных действий состоит из 5 действий: [ничего не делать, влево, вправо, газ, тормоз].

## Выводы:

1. Сравнительный анализ двух алгоритмов обучения с подкреплением (SAC и DDPG) показал следующее:

* алгоритм мягких критических факторов (SAC) хорошо справляется с балансом между исследованиями и использованием.
* алгоритм SAC помогаат бороться с проблемой чрезмерного доверия к действиям агента.
* алгоритм DDPG подходит для задач с непрерывными пространствами действий.
* на производительность алгоритма глубкоих детерминированных градиентов политики (DDPG) может существенно повлиять выбор гиперпараметров (например, скорость обучения).

2. Изучены стратегии разведки и эксплуатации.
