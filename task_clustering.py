#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
–ó–ê–î–ê–ß–ê 6: –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–Ø –° –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï–ú KG EMBEDDINGS

–í—ã–ø–æ–ª–Ω—è–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Ö –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π.
–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—é (Manufacturer).
"""

from owlready2 import *
import os
import json
import numpy as np

try:
    import torch
    from pykeen.models import TransE
    HAS_PYKEEN = True
except ImportError:
    HAS_PYKEEN = False

try:
    from sklearn.cluster import KMeans
    from sklearn.decomposition import PCA
    from sklearn.metrics import adjusted_rand_score
    from sklearn.preprocessing import LabelEncoder
    HAS_SKLEARN = True
except ImportError:
    HAS_SKLEARN = False
    print("‚ùå sklearn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install scikit-learn")

try:
    import matplotlib.pyplot as plt
    HAS_MATPLOTLIB = True
except ImportError:
    HAS_MATPLOTLIB = False
    print("‚ö†Ô∏è matplotlib –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞.")


def load_model_and_embeddings(model_dir: str = "kg_embeddings_pykeen"):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç embeddings"""
    print("üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
    
    model_path = os.path.join(model_dir, "trained_model.pkl")
    model = torch.load(model_path, map_location='cpu', weights_only=False)
    
    with open(os.path.join(model_dir, "entity_to_id.json"), 'r') as f:
        entity_to_id = json.load(f)
    
    id_to_entity = {v: k for k, v in entity_to_id.items()}
    
    with torch.no_grad():
        all_entity_ids = torch.arange(model.num_entities, dtype=torch.long)
        embeddings = model.entity_representations[0](all_entity_ids).numpy()
    
    print(f"   ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(embeddings)} embeddings —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ {embeddings.shape[1]}")
    
    return model, entity_to_id, id_to_entity, embeddings


def prepare_clustering_data(onto, entity_to_id, embeddings, max_samples=1000):
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"""
    print("\nüìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏...")
    
    vehicle_embeddings = []
    vehicle_names = []
    vehicle_manufacturers = []
    
    vehicles = list(onto.Vehicle.instances())[:max_samples]
    
    for vehicle in vehicles:
        if vehicle.name not in entity_to_id:
            continue
        
        if not hasattr(vehicle, 'MadeBy') or not vehicle.MadeBy:
            continue
        
        entity_id = entity_to_id[vehicle.name]
        manufacturer = vehicle.MadeBy[0].name
        
        vehicle_embeddings.append(embeddings[entity_id])
        vehicle_names.append(vehicle.name)
        vehicle_manufacturers.append(manufacturer)
    
    X = np.array(vehicle_embeddings)
    
    top_manufacturers = {}
    for m in vehicle_manufacturers:
        top_manufacturers[m] = top_manufacturers.get(m, 0) + 1
    
    top_5 = sorted(top_manufacturers.items(), key=lambda x: -x[1])[:5]
    top_5_names = [m[0] for m in top_5]
    
    filtered_X = []
    filtered_names = []
    filtered_manufacturers = []
    
    for i, m in enumerate(vehicle_manufacturers):
        if m in top_5_names:
            filtered_X.append(X[i])
            filtered_names.append(vehicle_names[i])
            filtered_manufacturers.append(m)
    
    X = np.array(filtered_X)
    
    print(f"   –í—Å–µ–≥–æ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π: {len(filtered_X)}")
    print(f"   –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–∏ (—Ç–æ–ø-5): {top_5_names}")
    print(f"   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:")
    for m, count in top_5:
        print(f"      {m}: {count}")
    
    return X, filtered_names, filtered_manufacturers, top_5_names


def perform_clustering(X, true_labels, n_clusters):
    """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"""
    print(f"\nüîÑ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è KMeans (k={n_clusters})...")
    
    pca = PCA(n_components=2)
    X_2d = pca.fit_transform(X)
    print(f"   PCA: –æ–±—ä—è—Å–Ω–µ–Ω–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è = {sum(pca.explained_variance_ratio_)*100:.1f}%")
    
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    predicted_labels = kmeans.fit_predict(X)
    
    le = LabelEncoder()
    true_labels_encoded = le.fit_transform(true_labels)
    
    ari = adjusted_rand_score(true_labels_encoded, predicted_labels)
    print(f"   ‚úÖ Adjusted Rand Score: {ari:.4f}")
    
    return X_2d, predicted_labels, true_labels_encoded, ari, le


def visualize_clustering(X_2d, true_labels_encoded, predicted_labels, label_encoder, 
                        true_labels_original, output_prefix="clustering"):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"""
    if not HAS_MATPLOTLIB:
        print("‚ö†Ô∏è matplotlib –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–ø—É—â–µ–Ω–∞")
        return
    
    print("\nüìà –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π...")
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))
    
    ax1 = axes[0]
    scatter1 = ax1.scatter(X_2d[:, 0], X_2d[:, 1], c=true_labels_encoded, 
                          cmap='tab10', alpha=0.6, s=30)
    ax1.set_title('–û–∂–∏–¥–∞–µ–º—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã (–ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—é)', fontsize=12)
    ax1.set_xlabel('PCA Component 1')
    ax1.set_ylabel('PCA Component 2')
    
    unique_labels = sorted(set(true_labels_original))
    handles1 = [plt.scatter([], [], c=plt.cm.tab10(i/len(unique_labels)), label=label, s=50) 
               for i, label in enumerate(unique_labels)]
    ax1.legend(handles=handles1, title='–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å', loc='best', fontsize=8)
    
    ax2 = axes[1]
    scatter2 = ax2.scatter(X_2d[:, 0], X_2d[:, 1], c=predicted_labels, 
                          cmap='tab10', alpha=0.6, s=30)
    ax2.set_title('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã (KMeans)', fontsize=12)
    ax2.set_xlabel('PCA Component 1')
    ax2.set_ylabel('PCA Component 2')
    
    handles2 = [plt.scatter([], [], c=plt.cm.tab10(i/len(set(predicted_labels))), 
                           label=f'–ö–ª–∞—Å—Ç–µ—Ä {i}', s=50) 
               for i in sorted(set(predicted_labels))]
    ax2.legend(handles=handles2, title='–ö–ª–∞—Å—Ç–µ—Ä', loc='best', fontsize=8)
    
    plt.tight_layout()
    
    output_file = f"{output_prefix}_results.png"
    plt.savefig(output_file, dpi=150, bbox_inches='tight')
    print(f"   ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {output_file}")
    
    plt.close()


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("="*80)
    print("–ó–ê–î–ê–ß–ê 6: –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–Ø –° –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï–ú KG EMBEDDINGS")
    print("="*80)
    
    if not HAS_PYKEEN or not HAS_SKLEARN:
        print("‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏")
        return
    
    print("\nüìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –æ–Ω—Ç–æ–ª–æ–≥–∏–∏...")
    onto = get_ontology("file://" + os.path.abspath("cars_ontology.owl")).load()
    print(f"   ‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π: {len(list(onto.Vehicle.instances()))}")
    
    model, entity_to_id, id_to_entity, embeddings = load_model_and_embeddings()
    
    X, vehicle_names, vehicle_manufacturers, top_manufacturers = prepare_clustering_data(
        onto, entity_to_id, embeddings, max_samples=2000
    )
    
    n_clusters = len(top_manufacturers)
    
    X_2d, predicted_labels, true_labels_encoded, ari, label_encoder = perform_clustering(
        X, vehicle_manufacturers, n_clusters
    )
    
    visualize_clustering(
        X_2d, true_labels_encoded, predicted_labels, label_encoder,
        vehicle_manufacturers, output_prefix="clustering"
    )
    
    print("\n" + "="*80)
    print("–ò–¢–û–ì–ò –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–ò")
    print("="*80)
    
    print(f"""
üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
   - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {n_clusters}
   - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—ä–µ–∫—Ç–æ–≤: {len(X)}
   - –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å embeddings: {embeddings.shape[1]}
   - –ü—Ä–∏–∑–Ω–∞–∫ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å (Manufacturer)

üìà –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞:
   - Adjusted Rand Score: {ari:.4f}
   
üìå –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è Adjusted Rand Score:
   - ARI = 1.0: –∏–¥–µ–∞–ª—å–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
   - ARI = 0.0: —Å–ª—É—á–∞–π–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
   - ARI > 0.5: —Ö–æ—Ä–æ—à–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
   - ARI > 0.3: —É–º–µ—Ä–µ–Ω–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
   - ARI < 0.3: —Å–ª–∞–±–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è

‚úÖ –í—ã–≤–æ–¥: {"–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞!" if ari > 0.3 else "–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–ª–∞–±—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã."}
""")


if __name__ == "__main__":
    main()

