#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
–ó–ê–î–ê–ß–ê 7: –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø –° –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï–ú KG EMBEDDINGS

–í—ã–ø–æ–ª–Ω—è–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π –ø–æ —Ç–∏–ø—É –∫—É–∑–æ–≤–∞ (BodyStyle)
—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π.

–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç:
1. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ KG Embeddings
2. –ë–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å (most frequent class)
3. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ one-hot encoding
"""

from owlready2 import *
import os
import json
import numpy as np

try:
    import torch
    from pykeen.models import TransE
    HAS_PYKEEN = True
except ImportError:
    HAS_PYKEEN = False

try:
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import LabelEncoder, OneHotEncoder
    from sklearn.metrics import accuracy_score, classification_report
    from sklearn.linear_model import LogisticRegression
    from sklearn.ensemble import RandomForestClassifier
    HAS_SKLEARN = True
except ImportError:
    HAS_SKLEARN = False
    print("‚ùå sklearn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install scikit-learn")

try:
    from xgboost import XGBClassifier
    HAS_XGBOOST = True
except ImportError:
    HAS_XGBOOST = False
    print("‚ö†Ô∏è xgboost –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–µ–º RandomForest –≤–º–µ—Å—Ç–æ XGBoost.")


def load_model_and_embeddings(model_dir: str = "kg_embeddings_pykeen"):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç embeddings"""
    print("üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
    
    model_path = os.path.join(model_dir, "trained_model.pkl")
    model = torch.load(model_path, map_location='cpu', weights_only=False)
    
    with open(os.path.join(model_dir, "entity_to_id.json"), 'r') as f:
        entity_to_id = json.load(f)
    
    id_to_entity = {v: k for k, v in entity_to_id.items()}
    
    with torch.no_grad():
        all_entity_ids = torch.arange(model.num_entities, dtype=torch.long)
        embeddings = model.entity_representations[0](all_entity_ids).numpy()
    
    print(f"   ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(embeddings)} embeddings —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ {embeddings.shape[1]}")
    
    return model, entity_to_id, id_to_entity, embeddings


def prepare_classification_data(onto, entity_to_id, embeddings, max_samples=3000):
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
    print("\nüìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏...")
    
    vehicle_embeddings = []
    vehicle_names = []
    vehicle_body_styles = []
    vehicle_years = []
    vehicle_manufacturers = []
    
    vehicles = list(onto.Vehicle.instances())[:max_samples]
    
    for vehicle in vehicles:
        if vehicle.name not in entity_to_id:
            continue
        
        if not hasattr(vehicle, 'StyledAs') or not vehicle.StyledAs:
            continue
        
        if not hasattr(vehicle, 'Year') or not vehicle.Year:
            continue
        
        entity_id = entity_to_id[vehicle.name]
        body_style = vehicle.StyledAs[0].name
        year = vehicle.Year
        
        manufacturer = "Unknown"
        if hasattr(vehicle, 'MadeBy') and vehicle.MadeBy:
            manufacturer = vehicle.MadeBy[0].name
        
        vehicle_embeddings.append(embeddings[entity_id])
        vehicle_names.append(vehicle.name)
        vehicle_body_styles.append(body_style)
        vehicle_years.append(year)
        vehicle_manufacturers.append(manufacturer)
    
    style_counts = {}
    for s in vehicle_body_styles:
        style_counts[s] = style_counts.get(s, 0) + 1
    
    top_styles = sorted(style_counts.items(), key=lambda x: -x[1])[:5]
    top_style_names = [s[0] for s in top_styles]
    
    filtered_embeddings = []
    filtered_names = []
    filtered_styles = []
    filtered_years = []
    filtered_manufacturers = []
    
    for i, style in enumerate(vehicle_body_styles):
        if style in top_style_names:
            filtered_embeddings.append(vehicle_embeddings[i])
            filtered_names.append(vehicle_names[i])
            filtered_styles.append(style)
            filtered_years.append(vehicle_years[i])
            filtered_manufacturers.append(vehicle_manufacturers[i])
    
    print(f"   –í—Å–µ–≥–æ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π: {len(filtered_embeddings)}")
    print(f"   –ö–ª–∞—Å—Å—ã (—Ç–∏–ø—ã –∫—É–∑–æ–≤–∞): {top_style_names}")
    print(f"   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:")
    for s, count in top_styles:
        print(f"      {s}: {count}")
    
    return (np.array(filtered_embeddings), filtered_names, filtered_styles, 
            filtered_years, filtered_manufacturers, top_style_names)


def split_data_by_year(X, y, years, split_year=2015):
    """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ –≥–æ–¥—É (–∫—Ä–∏—Ç–µ—Ä–∏–π –∏–∑ –ø—É–Ω–∫—Ç–∞ 2.3)"""
    print(f"\nüìÖ –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ –≥–æ–¥—É (–≥—Ä–∞–Ω–∏—Ü–∞: {split_year})...")
    
    train_idx = [i for i, year in enumerate(years) if year < split_year]
    test_idx = [i for i, year in enumerate(years) if year >= split_year]
    
    X_train = X[train_idx]
    X_test = X[test_idx]
    y_train = [y[i] for i in train_idx]
    y_test = [y[i] for i in test_idx]
    
    print(f"   Train (–≥–æ–¥ < {split_year}): {len(X_train)} –æ–±—Ä–∞–∑—Ü–æ–≤")
    print(f"   Test (–≥–æ–¥ >= {split_year}): {len(X_test)} –æ–±—Ä–∞–∑—Ü–æ–≤")
    
    return X_train, X_test, y_train, y_test


def create_onehot_features(manufacturers, all_manufacturers):
    """–°–æ–∑–¥–∞–Ω–∏–µ one-hot encoding –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π"""
    manufacturer_to_idx = {m: i for i, m in enumerate(sorted(set(all_manufacturers)))}
    n_manufacturers = len(manufacturer_to_idx)
    
    onehot = np.zeros((len(manufacturers), n_manufacturers))
    for i, m in enumerate(manufacturers):
        if m in manufacturer_to_idx:
            onehot[i, manufacturer_to_idx[m]] = 1
    
    return onehot


def baseline_most_frequent(y_train, y_test):
    """–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: –≤—Å–µ–≥–¥–∞ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–π –∫–ª–∞—Å—Å"""
    from collections import Counter
    
    most_common = Counter(y_train).most_common(1)[0][0]
    predictions = [most_common] * len(y_test)
    
    return predictions, most_common


def train_and_evaluate(X_train, X_test, y_train, y_test, model_name="KG Embeddings"):
    """–û–±—É—á–µ–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
    print(f"\nüîÑ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ ({model_name})...")
    
    le = LabelEncoder()
    y_train_encoded = le.fit_transform(y_train)
    y_test_encoded = le.transform(y_test)
    
    if HAS_XGBOOST:
        clf = XGBClassifier(
            n_estimators=100,
            max_depth=5,
            learning_rate=0.1,
            random_state=42,
            use_label_encoder=False,
            eval_metric='mlogloss'
        )
    else:
        clf = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=42
        )
    
    clf.fit(X_train, y_train_encoded)
    
    y_pred_encoded = clf.predict(X_test)
    y_pred = le.inverse_transform(y_pred_encoded)
    
    accuracy = accuracy_score(y_test, y_pred)
    
    print(f"   ‚úÖ Accuracy: {accuracy:.4f} ({accuracy*100:.1f}%)")
    
    return accuracy, y_pred, le


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("="*80)
    print("–ó–ê–î–ê–ß–ê 7: –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø –° –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï–ú KG EMBEDDINGS")
    print("="*80)
    
    if not HAS_PYKEEN or not HAS_SKLEARN:
        print("‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏")
        return
    
    print("\nüìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –æ–Ω—Ç–æ–ª–æ–≥–∏–∏...")
    onto = get_ontology("file://" + os.path.abspath("cars_ontology.owl")).load()
    print(f"   ‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π: {len(list(onto.Vehicle.instances()))}")
    
    model, entity_to_id, id_to_entity, embeddings = load_model_and_embeddings()
    
    X, names, body_styles, years, manufacturers, style_names = prepare_classification_data(
        onto, entity_to_id, embeddings, max_samples=5000
    )
    
    X_train, X_test, y_train, y_test = split_data_by_year(X, body_styles, years, split_year=2015)
    
    train_idx = [i for i, year in enumerate(years) if year < 2015]
    test_idx = [i for i, year in enumerate(years) if year >= 2015]
    manufacturers_train = [manufacturers[i] for i in train_idx]
    manufacturers_test = [manufacturers[i] for i in test_idx]
    
    print("\n" + "="*80)
    print("–ú–û–î–ï–õ–¨ 1: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ KG Embeddings")
    print("="*80)
    
    acc_kg, pred_kg, le = train_and_evaluate(X_train, X_test, y_train, y_test, "KG Embeddings")
    
    print("\n" + "="*80)
    print("–ú–û–î–ï–õ–¨ 2: –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å (most frequent class)")
    print("="*80)
    
    pred_baseline, most_common = baseline_most_frequent(y_train, y_test)
    acc_baseline = accuracy_score(y_test, pred_baseline)
    print(f"   –ù–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–π –∫–ª–∞—Å—Å: {most_common}")
    print(f"   ‚úÖ Accuracy: {acc_baseline:.4f} ({acc_baseline*100:.1f}%)")
    
    print("\n" + "="*80)
    print("–ú–û–î–ï–õ–¨ 3: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ One-Hot Encoding")
    print("="*80)
    
    X_onehot_train = create_onehot_features(manufacturers_train, manufacturers)
    X_onehot_test = create_onehot_features(manufacturers_test, manufacturers)
    
    print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å one-hot: {X_onehot_train.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    
    acc_onehot, pred_onehot, _ = train_and_evaluate(
        X_onehot_train, X_onehot_test, y_train, y_test, "One-Hot Encoding"
    )
    
    print("\n" + "="*80)
    print("–°–†–ê–í–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
    print("="*80)
    
    print(f"""
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ –ú–æ–¥–µ–ª—å                     ‚îÇ Accuracy   ‚îÇ –£–ª—É—á—à–µ–Ω–∏–µ   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ –ë–∞–∑–æ–≤–∞—è (most frequent)    ‚îÇ {acc_baseline*100:>6.1f}%    ‚îÇ    -        ‚îÇ
‚îÇ One-Hot Encoding           ‚îÇ {acc_onehot*100:>6.1f}%    ‚îÇ {(acc_onehot-acc_baseline)*100:>+5.1f}%     ‚îÇ
‚îÇ KG Embeddings              ‚îÇ {acc_kg*100:>6.1f}%    ‚îÇ {(acc_kg-acc_baseline)*100:>+5.1f}%     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
""")
    
    print("üìå –í—ã–≤–æ–¥:")
    if acc_kg > acc_baseline:
        improvement = (acc_kg - acc_baseline) * 100
        print(f"   ‚úÖ KG Embeddings —É–ª—É—á—à–∏–ª–∏ —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ {improvement:.1f}% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é!")
    else:
        print(f"   ‚ö†Ô∏è KG Embeddings –Ω–µ —É–ª—É—á—à–∏–ª–∏ —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é.")
    
    if acc_kg > acc_onehot:
        improvement = (acc_kg - acc_onehot) * 100
        print(f"   ‚úÖ KG Embeddings –ª—É—á—à–µ One-Hot Encoding –Ω–∞ {improvement:.1f}%")
    else:
        diff = (acc_onehot - acc_kg) * 100
        print(f"   ‚ö†Ô∏è One-Hot Encoding –ª—É—á—à–µ KG Embeddings –Ω–∞ {diff:.1f}%")
    
    print("\n" + "="*80)
    print("–î–ï–¢–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ –ü–û –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò (KG Embeddings)")
    print("="*80)
    
    from sklearn.metrics import classification_report
    print(classification_report(y_test, pred_kg, target_names=style_names, zero_division=0))


if __name__ == "__main__":
    main()

