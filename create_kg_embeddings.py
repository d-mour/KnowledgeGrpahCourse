#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Knowledge Graph Embeddings (KGE) –¥–ª—è –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π

–†–µ–∞–ª–∏–∑—É–µ—Ç –ø–æ–¥—Ö–æ–¥ –∏–∑ –ª–µ–∫—Ü–∏–∏:
- –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç—Ä–∏–ø–ª–µ—Ç—ã (subject, predicate, object) –∏–∑ –≥—Ä–∞—Ñ–∞
- –°–æ–∑–¥–∞–µ—Ç embeddings –¥–ª—è —Å—É—â–Ω–æ—Å—Ç–µ–π (entities) –∏ –æ—Ç–Ω–æ—à–µ–Ω–∏–π (relations)
- –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –∑–∞–¥–∞—á–µ link prediction
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–¥–µ–ª—å TransE (–ø—Ä–æ—Å—Ç–∞—è –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è)
"""

from owlready2 import *
import numpy as np
import random
import json
import pickle
import os
from typing import List, Tuple, Dict, Set, Optional
from collections import defaultdict
import math

# –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–Ω—Ç–æ–ª–æ–≥–∏—é
onto = get_ontology("file://cars_ontology.owl").load()

# –ë–∞–∑–æ–≤—ã–π namespace
BASE_NS = "http://www.semanticweb.org/fqy/ontologies/2025/9/untitled-ontology-7#"


def extract_triples_from_ontology() -> List[Tuple[str, str, str]]:
    """
    –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –¢–†–ò–ü–õ–ï–¢–û–í –ò–ó –ì–†–ê–§–ê –ó–ù–ê–ù–ò–ô
    
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ —Ç—Ä–∏–ø–ª–µ—Ç—ã –≤–∏–¥–∞ (subject, predicate, object) –∏–∑ –æ–Ω—Ç–æ–ª–æ–≥–∏–∏.
    –≠—Ç–æ –æ—Å–Ω–æ–≤–∞ –¥–ª—è Knowledge Graph Embeddings.
    
    –¢—Ä–∏–ø–ª–µ—Ç—ã:
    - (Vehicle, MadeBy, Manufacturer)
    - (Vehicle, StyledAs, BodyStyle)
    - (Vehicle, hasSegment, MarketSegment)
    - (Vehicle, hasEngine, Engine)
    - (Manufacturer, WhereIs, Country)
    –∏ —Ç.–¥.
    
    Returns:
        List[Tuple[str, str, str]]: —Å–ø–∏—Å–æ–∫ —Ç—Ä–∏–ø–ª–µ—Ç–æ–≤ (head, relation, tail)
    """
    print("="*80)
    print("–ò–ó–í–õ–ï–ß–ï–ù–ò–ï –¢–†–ò–ü–õ–ï–¢–û–í –ò–ó –ì–†–ê–§–ê –ó–ù–ê–ù–ò–ô")
    print("="*80)
    
    triples = []
    
    # 1. Vehicle --MadeBy--> Manufacturer
    print("\n1. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–≤—è–∑–µ–π Vehicle --MadeBy--> Manufacturer...")
    for vehicle in onto.Vehicle.instances():
        if hasattr(vehicle, 'MadeBy') and vehicle.MadeBy:
            for manufacturer in vehicle.MadeBy:
                triples.append((vehicle.name, 'MadeBy', manufacturer.name))
    
    # 2. Vehicle --StyledAs--> BodyStyle
    print("2. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–≤—è–∑–µ–π Vehicle --StyledAs--> BodyStyle...")
    for vehicle in onto.Vehicle.instances():
        if hasattr(vehicle, 'StyledAs') and vehicle.StyledAs:
            for body_style in vehicle.StyledAs:
                triples.append((vehicle.name, 'StyledAs', body_style.name))
    
    # 3. Vehicle --hasSegment--> MarketSegment
    print("3. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–≤—è–∑–µ–π Vehicle --hasSegment--> MarketSegment...")
    for vehicle in onto.Vehicle.instances():
        if hasattr(vehicle, 'hasSegment') and vehicle.hasSegment:
            for segment in vehicle.hasSegment:
                triples.append((vehicle.name, 'hasSegment', segment.name))
    
    # 4. Vehicle --hasEngine--> Engine
    print("4. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–≤—è–∑–µ–π Vehicle --hasEngine--> Engine...")
    for vehicle in onto.Vehicle.instances():
        if hasattr(vehicle, 'hasEngine') and vehicle.hasEngine:
            for engine in vehicle.hasEngine:
                triples.append((vehicle.name, 'hasEngine', engine.name))
    
    # 5. Vehicle --hasTransmission--> Transmission
    print("5. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–≤—è–∑–µ–π Vehicle --hasTransmission--> Transmission...")
    for vehicle in onto.Vehicle.instances():
        if hasattr(vehicle, 'hasTransmission') and vehicle.hasTransmission:
            for transmission in vehicle.hasTransmission:
                triples.append((vehicle.name, 'hasTransmission', transmission.name))
    
    # 6. Manufacturer --WhereIs--> Country
    print("6. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–≤—è–∑–µ–π Manufacturer --WhereIs--> Country...")
    for manufacturer in onto.Manufacturer.instances():
        if hasattr(manufacturer, 'WhereIs') and manufacturer.WhereIs:
            for country in manufacturer.WhereIs:
                triples.append((manufacturer.name, 'WhereIs', country.name))
    
    # 7. Engine --MadeBy--> Manufacturer (–µ—Å–ª–∏ –µ—Å—Ç—å)
    print("7. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–≤—è–∑–µ–π Engine --MadeBy--> Manufacturer...")
    for engine in onto.Engine.instances():
        if hasattr(engine, 'MadeBy') and engine.MadeBy:
            for manufacturer in engine.MadeBy:
                triples.append((engine.name, 'MadeBy', manufacturer.name))
    
    print(f"\n‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ —Ç—Ä–∏–ø–ª–µ—Ç–æ–≤: {len(triples)}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    relations_count = defaultdict(int)
    for _, rel, _ in triples:
        relations_count[rel] += 1
    
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –æ—Ç–Ω–æ—à–µ–Ω–∏—è–º:")
    for rel, count in sorted(relations_count.items(), key=lambda x: -x[1]):
        print(f"   {rel}: {count}")
    
    return triples


def create_entity_relation_mappings(triples: List[Tuple[str, str, str]]) -> Tuple[Dict[str, int], Dict[str, int], Dict[int, str], Dict[int, str]]:
    """
    –°–û–ó–î–ê–ù–ò–ï –ò–ù–î–ï–ö–°–û–í –î–õ–Ø –°–£–©–ù–û–°–¢–ï–ô –ò –û–¢–ù–û–®–ï–ù–ò–ô
    
    –°–æ–∑–¥–∞–µ—Ç –º–∞–ø–ø–∏–Ω–≥–∏:
    - entity_to_id: –∏–º—è —Å—É—â–Ω–æ—Å—Ç–∏ -> ID
    - relation_to_id: –∏–º—è –æ—Ç–Ω–æ—à–µ–Ω–∏—è -> ID
    - id_to_entity: ID -> –∏–º—è —Å—É—â–Ω–æ—Å—Ç–∏ (–æ–±—Ä–∞—Ç–Ω—ã–π)
    - id_to_relation: ID -> –∏–º—è –æ—Ç–Ω–æ—à–µ–Ω–∏—è (–æ–±—Ä–∞—Ç–Ω—ã–π)
    
    –≠—Ç–æ –Ω—É–∂–Ω–æ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å embeddings (–≤–º–µ—Å—Ç–æ —Å—Ç—Ä–æ–∫ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å—ã)
    """
    print("\n" + "="*80)
    print("–°–û–ó–î–ê–ù–ò–ï –ò–ù–î–ï–ö–°–û–í –î–õ–Ø –°–£–©–ù–û–°–¢–ï–ô –ò –û–¢–ù–û–®–ï–ù–ò–ô")
    print("="*80)
    
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ –∏ –æ—Ç–Ω–æ—à–µ–Ω–∏—è
    entities = set()
    relations = set()
    
    for head, rel, tail in triples:
        entities.add(head)
        entities.add(tail)
        relations.add(rel)
    
    # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥–∏
    entity_list = sorted(list(entities))
    relation_list = sorted(list(relations))
    
    entity_to_id = {entity: idx for idx, entity in enumerate(entity_list)}
    relation_to_id = {rel: idx for idx, rel in enumerate(relation_list)}
    
    id_to_entity = {idx: entity for entity, idx in entity_to_id.items()}
    id_to_relation = {idx: rel for rel, idx in relation_to_id.items()}
    
    print(f"\n‚úÖ –°—É—â–Ω–æ—Å—Ç–µ–π: {len(entity_to_id)}")
    print(f"‚úÖ –û—Ç–Ω–æ—à–µ–Ω–∏–π: {len(relation_to_id)}")
    print(f"\nüìã –û—Ç–Ω–æ—à–µ–Ω–∏—è: {', '.join(relation_list)}")
    
    return entity_to_id, relation_to_id, id_to_entity, id_to_relation


def convert_triples_to_ids(triples: List[Tuple[str, str, str]], 
                          entity_to_id: Dict[str, int],
                          relation_to_id: Dict[str, int]) -> List[Tuple[int, int, int]]:
    """
    –ö–û–ù–í–ï–†–¢–ê–¶–ò–Ø –¢–†–ò–ü–õ–ï–¢–û–í –í –ò–ù–î–ï–ö–°–´
    
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ç—Ä–∏–ø–ª–µ—Ç—ã –∏–∑ —Å—Ç—Ä–æ–∫ –≤ —á–∏—Å–ª–æ–≤—ã–µ –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
    """
    id_triples = []
    for head, rel, tail in triples:
        if head in entity_to_id and tail in entity_to_id and rel in relation_to_id:
            id_triples.append((entity_to_id[head], relation_to_id[rel], entity_to_id[tail]))
    
    return id_triples


class TransE:
    """
    –ú–û–î–ï–õ–¨ TransE (Translating Embeddings)
    
    –ü—Ä–æ—Å—Ç–∞—è –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è Knowledge Graph Embeddings.
    
    –ò–¥–µ—è:
    - –ö–∞–∂–¥–∞—è —Å—É—â–Ω–æ—Å—Ç—å (entity) –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –≤–µ–∫—Ç–æ—Ä–æ–º
    - –ö–∞–∂–¥–æ–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–µ (relation) –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ –≤–µ–∫—Ç–æ—Ä–æ–º –ø–µ—Ä–µ–≤–æ–¥–∞
    - –î–ª—è —Ç—Ä–∏–ø–ª–µ—Ç–∞ (h, r, t): h + r ‚âà t
    
    Score function: -||h + r - t||
    –ß–µ–º –≤—ã—à–µ score, —Ç–µ–º –±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–µ–Ω —Ç—Ä–∏–ø–ª–µ—Ç
    """
    
    def __init__(self, num_entities: int, num_relations: int, embedding_dim: int = 64):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        
        Args:
            num_entities: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—É—â–Ω–æ—Å—Ç–µ–π
            num_relations: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–Ω–æ—à–µ–Ω–∏–π
            embedding_dim: —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å embeddings
        """
        self.num_entities = num_entities
        self.num_relations = num_relations
        self.embedding_dim = embedding_dim
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è embeddings (Xavier uniform)
        bound = 6.0 / math.sqrt(embedding_dim)
        self.entity_embeddings = np.random.uniform(-bound, bound, (num_entities, embedding_dim))
        self.relation_embeddings = np.random.uniform(-bound, bound, (num_relations, embedding_dim))
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        self.entity_embeddings = self.entity_embeddings / np.linalg.norm(self.entity_embeddings, axis=1, keepdims=True)
        self.relation_embeddings = self.relation_embeddings / np.linalg.norm(self.relation_embeddings, axis=1, keepdims=True)
        
        print(f"\n‚úÖ –ú–æ–¥–µ–ª—å TransE –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞:")
        print(f"   –°—É—â–Ω–æ—Å—Ç–µ–π: {num_entities}")
        print(f"   –û—Ç–Ω–æ—à–µ–Ω–∏–π: {num_relations}")
        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å embeddings: {embedding_dim}")
    
    def score(self, h: int, r: int, t: int) -> float:
        """
        –í–´–ß–ò–°–õ–ï–ù–ò–ï SCORE –î–õ–Ø –¢–†–ò–ü–õ–ï–¢–ê
        
        Score = -||h + r - t||
        –ß–µ–º –≤—ã—à–µ score, —Ç–µ–º –±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–µ–Ω —Ç—Ä–∏–ø–ª–µ—Ç
        """
        h_vec = self.entity_embeddings[h]
        r_vec = self.relation_embeddings[r]
        t_vec = self.entity_embeddings[t]
        
        # h + r - t
        diff = h_vec + r_vec - t_vec
        
        # L2 –Ω–æ—Ä–º–∞ (—Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ)
        distance = np.linalg.norm(diff)
        
        # Score (–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ - —á–µ–º –±–ª–∏–∂–µ, —Ç–µ–º –≤—ã—à–µ score)
        return -distance
    
    def train(self, triples: List[Tuple[int, int, int]], 
              num_epochs: int = 100,
              learning_rate: float = 0.0001,
              margin: float = 1.0,
              negative_ratio: int = 1,
              batch_size: int = 1000):
        """
        –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç margin ranking loss —Å negative sampling (sLCWA –∏–∑ –ª–µ–∫—Ü–∏–∏)
        
        Loss = max(0, margin - score(positive) + score(negative))
        """
        print("\n" + "="*80)
        print("–û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò TransE")
        print("="*80)
        print(f"   –≠–ø–æ—Ö: {num_epochs}")
        print(f"   Learning rate: {learning_rate}")
        print(f"   Margin: {margin}")
        print(f"   Negative ratio: {negative_ratio}")
        print(f"   Batch size: {batch_size}")
        
        # –°–æ–∑–¥–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–æ –≤—Å–µ—Ö —Ç—Ä–∏–ø–ª–µ—Ç–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
        triple_set = set(triples)
        
        # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π –¥–ª—è negative sampling
        all_entities = list(range(self.num_entities))
        
        for epoch in range(num_epochs):
            total_loss = 0.0
            num_batches = 0
            
            # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º —Ç—Ä–∏–ø–ª–µ—Ç—ã
            shuffled_triples = triples.copy()
            random.shuffle(shuffled_triples)
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á–∞–º–∏
            for i in range(0, len(shuffled_triples), batch_size):
                batch = shuffled_triples[i:i+batch_size]
                
                batch_loss = 0.0
                
                for h, r, t in batch:
                    # Positive triple score
                    pos_score = self.score(h, r, t)
                    
                    # Negative sampling
                    for _ in range(negative_ratio):
                        # Corrupt tail (–∑–∞–º–µ–Ω—è–µ–º tail –Ω–∞ —Å–ª—É—á–∞–π–Ω—É—é —Å—É—â–Ω–æ—Å—Ç—å)
                        neg_t = random.choice(all_entities)
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ –Ω–∞—Å—Ç–æ—è—â–∏–π —Ç—Ä–∏–ø–ª–µ—Ç
                        while (h, r, neg_t) in triple_set:
                            neg_t = random.choice(all_entities)
                        
                        neg_score = self.score(h, r, neg_t)
                        
                        # Margin ranking loss
                        loss = max(0, margin - pos_score + neg_score)
                        batch_loss += loss
                        
                        if loss > 0:
                            # –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ –¥–ª—è TransE
                            # Loss = max(0, margin - score(pos) + score(neg))
                            # score = -||h + r - t||
                            # –ì—Ä–∞–¥–∏–µ–Ω—Ç score –ø–æ h: -(h + r - t) / ||h + r - t||
                            
                            h_vec = self.entity_embeddings[h].copy()
                            r_vec = self.relation_embeddings[r].copy()
                            t_pos_vec = self.entity_embeddings[t].copy()
                            t_neg_vec = self.entity_embeddings[neg_t].copy()
                            
                            # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–Ω–æ—Å—Ç–∏
                            diff_pos = h_vec + r_vec - t_pos_vec
                            diff_neg = h_vec + r_vec - t_neg_vec
                            
                            norm_pos = np.linalg.norm(diff_pos)
                            norm_neg = np.linalg.norm(diff_neg)
                            
                            # –ò–∑–±–µ–≥–∞–µ–º –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å
                            if norm_pos < 1e-8:
                                norm_pos = 1e-8
                            if norm_neg < 1e-8:
                                norm_neg = 1e-8
                            
                            # –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã (—Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –∑–Ω–∞–∫–æ–º –¥–ª—è –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ loss)
                            # –î–ª—è loss = margin - score(pos) + score(neg)
                            # grad = -grad_score(pos) + grad_score(neg)
                            # grad_score = -diff / norm (–¥–ª—è –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è)
                            
                            grad_h = learning_rate * (diff_pos / norm_pos - diff_neg / norm_neg)
                            grad_r = learning_rate * (diff_pos / norm_pos - diff_neg / norm_neg)
                            grad_t_pos = -learning_rate * diff_pos / norm_pos
                            grad_t_neg = learning_rate * diff_neg / norm_neg
                            
                            # –û–±–Ω–æ–≤–ª—è–µ–º embeddings
                            self.entity_embeddings[h] += grad_h
                            self.relation_embeddings[r] += grad_r
                            self.entity_embeddings[t] += grad_t_pos
                            self.entity_embeddings[neg_t] += grad_t_neg
                            
                            # –ù–ï –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è - —ç—Ç–æ –º–µ—à–∞–µ—Ç –æ–±—É—á–µ–Ω–∏—é
                            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –±—É–¥–µ—Ç —Ç–æ–ª—å–∫–æ –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –∏–ª–∏ –≤ –∫–æ–Ω—Ü–µ
                
                total_loss += batch_loss
                num_batches += 1
            
            avg_loss = total_loss / (num_batches * len(batch) * negative_ratio) if num_batches > 0 else 0
            
            # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (—Ä–∞–∑ –≤ 10 —ç–ø–æ—Ö) –≤–º–µ—Å—Ç–æ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
            if (epoch + 1) % 10 == 0:
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–æ–ª—å–∫–æ entity embeddings, —á—Ç–æ–±—ã –æ–Ω–∏ –Ω–µ "—Ä–∞–∑–±–µ–≥–∞–ª–∏—Å—å"
                norms = np.linalg.norm(self.entity_embeddings, axis=1, keepdims=True)
                norms[norms < 1e-8] = 1.0  # –ò–∑–±–µ–≥–∞–µ–º –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å
                self.entity_embeddings = self.entity_embeddings / norms
            
            # –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º –ª—É—á—à–∏–π loss
            if epoch == 0:
                best_loss = avg_loss
            elif avg_loss < best_loss:
                best_loss = avg_loss
            
            if (epoch + 1) % 10 == 0 or epoch == 0:
                trend = "üìâ" if avg_loss < best_loss else "üìà" if avg_loss > best_loss else "‚û°Ô∏è"
                print(f"   –≠–ø–æ—Ö–∞ {epoch + 1}/{num_epochs}: —Å—Ä–µ–¥–Ω–∏–π loss = {avg_loss:.4f} {trend} (–ª—É—á—à–∏–π: {best_loss:.4f})")
                
                # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –µ—Å–ª–∏ loss —Ä–∞—Å—Ç–µ—Ç
                if epoch > 10 and avg_loss > best_loss * 1.5:
                    print(f"      ‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: Loss —Ä–∞—Å—Ç–µ—Ç! –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–º–µ–Ω—å—à–∏—Ç—å learning rate –∏–ª–∏ —É–≤–µ–ª–∏—á–∏—Ç—å margin")
        
        print(f"\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    
    def predict_tail(self, h: int, r: int, top_k: int = 10) -> List[Tuple[int, float]]:
        """
        –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï TAIL –î–õ–Ø (head, relation, ?)
        
        –í—ã—á–∏—Å–ª—è–µ—Ç score –¥–ª—è –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö tail –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ø-k
        """
        scores = []
        for t in range(self.num_entities):
            score = self.score(h, r, t)
            scores.append((t, score))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é score
        scores.sort(key=lambda x: -x[1])
        
        return scores[:top_k]
    
    def predict_head(self, r: int, t: int, top_k: int = 10) -> List[Tuple[int, float]]:
        """
        –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï HEAD –î–õ–Ø (?, relation, tail)
        
        –í—ã—á–∏—Å–ª—è–µ—Ç score –¥–ª—è –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö head –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ø-k
        """
        scores = []
        for h in range(self.num_entities):
            score = self.score(h, r, t)
            scores.append((h, score))
        
        scores.sort(key=lambda x: -x[1])
        
        return scores[:top_k]


def create_kg_embeddings(limit: Optional[int] = None, 
                        embedding_dim: int = 64,
                        num_epochs: int = 50,
                        learning_rate: float = 0.0001):
    """
    –°–û–ó–î–ê–ù–ò–ï KNOWLEDGE GRAPH EMBEDDINGS
    
    –ü–æ–ª–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å:
    1. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç—Ä–∏–ø–ª–µ—Ç–æ–≤ –∏–∑ –≥—Ä–∞—Ñ–∞
    2. –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤
    3. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ TransE
    4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    """
    print("="*80)
    print("–°–û–ó–î–ê–ù–ò–ï KNOWLEDGE GRAPH EMBEDDINGS")
    print("="*80)
    
    # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç—Ä–∏–ø–ª–µ—Ç—ã
    triples = extract_triples_from_ontology()
    
    if limit:
        # –ë–µ—Ä–µ–º —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ –∏–∑ –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ –æ—Ç–Ω–æ—à–µ–Ω–∏–π
        from collections import defaultdict
        triples_by_relation = defaultdict(list)
        for triple in triples:
            triples_by_relation[triple[1]].append(triple)
        
        # –ë–µ—Ä–µ–º –ø–æ limit/N –æ—Ç–Ω–æ—à–µ–Ω–∏–π –∏–∑ –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞
        limit_per_relation = max(1, limit // len(triples_by_relation))
        limited_triples = []
        for rel, rel_triples in triples_by_relation.items():
            limited_triples.extend(rel_triples[:limit_per_relation])
        
        triples = limited_triples
        print(f"\n‚ö†Ô∏è  –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ: –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–æ {limit_per_relation} —Ç—Ä–∏–ø–ª–µ—Ç–æ–≤ –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞")
    
    if len(triples) == 0:
        print("‚ùå –¢—Ä–∏–ø–ª–µ—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
        return None
    
    # 2. –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã
    entity_to_id, relation_to_id, id_to_entity, id_to_relation = create_entity_relation_mappings(triples)
    
    # 3. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ç—Ä–∏–ø–ª–µ—Ç—ã –≤ –∏–Ω–¥–µ–∫—Å—ã
    print("\n" + "="*80)
    print("–ö–û–ù–í–ï–†–¢–ê–¶–ò–Ø –¢–†–ò–ü–õ–ï–¢–û–í –í –ò–ù–î–ï–ö–°–´")
    print("="*80)
    id_triples = convert_triples_to_ids(triples, entity_to_id, relation_to_id)
    print(f"‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ —Ç—Ä–∏–ø–ª–µ—Ç–æ–≤: {len(id_triples)}")
    
    # 4. –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    model = TransE(
        num_entities=len(entity_to_id),
        num_relations=len(relation_to_id),
        embedding_dim=embedding_dim
    )
    
    model.train(
        triples=id_triples,
        num_epochs=num_epochs,
        learning_rate=learning_rate,
        margin=1.0,
        negative_ratio=1,
        batch_size=min(1000, len(id_triples))
    )
    
    # 5. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    output_dir = "kg_embeddings"
    os.makedirs(output_dir, exist_ok=True)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º embeddings
    np.save(os.path.join(output_dir, "entity_embeddings.npy"), model.entity_embeddings)
    np.save(os.path.join(output_dir, "relation_embeddings.npy"), model.relation_embeddings)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å—ã
    with open(os.path.join(output_dir, "entity_to_id.json"), 'w', encoding='utf-8') as f:
        json.dump(entity_to_id, f, ensure_ascii=False, indent=2)
    
    with open(os.path.join(output_dir, "relation_to_id.json"), 'w', encoding='utf-8') as f:
        json.dump(relation_to_id, f, ensure_ascii=False, indent=2)
    
    with open(os.path.join(output_dir, "id_to_entity.json"), 'w', encoding='utf-8') as f:
        json.dump(id_to_entity, f, ensure_ascii=False, indent=2)
    
    with open(os.path.join(output_dir, "id_to_relation.json"), 'w', encoding='utf-8') as f:
        json.dump(id_to_relation, f, ensure_ascii=False, indent=2)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å (–¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è)
    model_data = {
        'num_entities': model.num_entities,
        'num_relations': model.num_relations,
        'embedding_dim': model.embedding_dim
    }
    with open(os.path.join(output_dir, "model_info.json"), 'w') as f:
        json.dump(model_data, f, indent=2)
    
    print(f"\n‚úÖ –í—Å–µ —Ñ–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {output_dir}/")
    print(f"   - entity_embeddings.npy: embeddings —Å—É—â–Ω–æ—Å—Ç–µ–π")
    print(f"   - relation_embeddings.npy: embeddings –æ—Ç–Ω–æ—à–µ–Ω–∏–π")
    print(f"   - entity_to_id.json: –∏–Ω–¥–µ–∫—Å —Å—É—â–Ω–æ—Å—Ç–µ–π")
    print(f"   - relation_to_id.json: –∏–Ω–¥–µ–∫—Å –æ—Ç–Ω–æ—à–µ–Ω–∏–π")
    
    return model, entity_to_id, relation_to_id, id_to_entity, id_to_relation


if __name__ == "__main__":
    import sys
    
    limit = None
    embedding_dim = 64
    num_epochs = 50
    
    if len(sys.argv) > 1:
        if '--limit' in sys.argv:
            limit_idx = sys.argv.index('--limit')
            if limit_idx + 1 < len(sys.argv):
                limit = int(sys.argv[limit_idx + 1])
        
        if '--dim' in sys.argv:
            dim_idx = sys.argv.index('--dim')
            if dim_idx + 1 < len(sys.argv):
                embedding_dim = int(sys.argv[dim_idx + 1])
        
        if '--epochs' in sys.argv:
            epochs_idx = sys.argv.index('--epochs')
            if epochs_idx + 1 < len(sys.argv):
                num_epochs = int(sys.argv[epochs_idx + 1])
    
    create_kg_embeddings(
        limit=limit,
        embedding_dim=embedding_dim,
        num_epochs=num_epochs
    )

